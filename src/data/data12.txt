Paragraphs:
On this page
You’ve decided how to format your data, and collected it usingSegment Sources. Now what do you do with it? You send the data to Destinations.
Destinations are tools or services which can use the data sent from Segment to power analytics, marketing, customer outreach, and more.
Each Segment Workspace has its own set of destinations, which are connected to the workspace’s sources. When you add or modify a destination, make sure you’re working with the correct workspace.
Healthcare and Life Sciences (HLS) customers can encrypt data flowing into their destinations
HLS customers with a HIPAA eligible workspace can encrypt data in fields marked as Yellow in the Privacy Portal before they flow into an event stream, cloud-mode destination.
To learn more about data encryption, see theHIPAA Eligible Segment documentation.
There are two ways to add a destination to your deployment: using the Segment web app, or using thePublic API.
Some third-party tools (such as Customer.io, Leanplum, and Airship) can both consume Segment data (as destinations), and send their data to Segment Warehouses asCloud-Sources. When you add a destination, make sure you’re viewing the Destinations tab in the catalog so you add the correct one.
If you have more than one instance of the same destination, you can clickCopy Settings From Other Destinationto save yourself time entering the settings values manually.
You can also add a destination directly from the source’s settings page in the Segment web app.
You can use the Segment Public API to add destinations to your workspace using theCreate Destination endpoint. The API requires an authorization token, and uses thenamefield as a namespace that defines whichsourcethe destination is connected to. You send the rest of the destination’s configuration as a JSON blob. View the documentation page in the Segment Catalog, or query theSegment Catalog API, for a destination to see the available settings.
You must use an authorization token to access the Public API, and these tokens are tied to specific workspaces. If you use the Public API to access multiple workspaces, make sure you’re using the token for the workspace you want to access before proceeding.
Adding a destination can have a few different effects, depending on which sources you set up to collect your data, and how you configured them.
If you are usingSegment’s JavaScript library, Analytics.js, then Segment handles any configuration changes you need for you. If you’re using Analytics.js in cloud-mode, the library sends its tracking data to the Segment servers, which route it to your destinations. When you change which destinations you send data to, the Segment servers automatically add that destination to the distribution list.
If you’re using Analytics.js in device-mode, then Analytics.js serves as a wrapper around additional code used by the individual destinations to run on the user’s device. When you add a destination, the Segment servers update a list of destinations that the library queries. When a user next loads your site, Analytics.js checks the list of destinations to load code for, and adds the new destination’s code to what it loads. It can take up to 30 minutes for the list to update, due to CDN caching.
You can enable device-mode for some destinations from the destination’s Settings page in the Segment web app. You don’t need to use the same mode for all destinations in a workspace; some can use device-mode, and some can use cloud-mode.
By default, Segment’smobile sourcessend data to Segment in cloud-mode to help minimize the size of your apps. In cloud-mode the mobile source libraries forward the tracking data to the Segment servers, which route the data to the destinations. Since the Segment servers know which destinations you’re using, you don’t need to take any action to add destinations to mobile apps using cloud-mode.
However, if the destination you’re adding has features that run on the user’s device, you might need to update the app to package that destination’s SDK with the library. Some destinations require that you package the SDK, and some only offer it
Segment’sserver sourcesrun on your internal app code, and never have access to the user’s device. They run in cloud-mode only, and forward their tracking calls to the Segment servers, which forward the data to any destinations you enabled.
When you add a destination in Segment, you must tell Segment how to connect with that destination’s app or endpoints. Most destinations offer an API token or authentication code which you can get from their web app. The documentation for each Segment destination includes information about what you need, and how to find it. Copy this information, and paste it into the Settings for the destination, or include it in thecreate API call.
Each destination can also have destination settings. These control how Segment transforms the data you send to the destination, and can be used to adapt it to your configuration or enable or disable certain destination features.
Multiple-destination support is available for all Segment customers on all plan tiers.
Segment allows you to connect a source to multiple instances of a destination. You can use this to set up a single Segment source that sends data into different instances of your analytics and other tools.
For example, you might set up a single Segment source to send data both to separate instances of Google Analytics for each business unit in your organization, and to another instance for executive-level reporting. You could also use this to make sure that tooling instances for different geographic teams are populated with the same data from the same source.
You can also connect multiple instances of a destination to help you smoothly migrate from one configuration to another. By sending each version the same data, you can check and validate the new configuration without interrupting use of the old one.
However, there are a few considerations:
Device-mode destinations do not support connecting multiple instances of the destination to the same source. If you try to a connect an additional instance of a device-mode destination to your source, the option to add a second instance does not appear.
Mobile sources, and the legacy Project source, can connect to multiple instances of destinations that operate only in cloud-mode. Mobile and Project sources cannot connect to multiple instances of destinations that operate in both cloud-mode and device-mode. Non-mobile sources can only connect toonedevice-mode instance of a destination.
Multi-instance support is not available for most hybrid Actions destinations or Web mode Actions destinations.
Segment does not support connecting a single source to multiple instances of aData Lakesdestination.
Non-mobile sources can only connect to _one_ device-mode instance of a destination
You cannot connect a source to more than one instance of a destination that operates only in device-mode. For more information about device-mode restrictions, see theSending Segment data to Destinationsdocumentation.
If your organization is on a Segment Business tier plan, you can useReplayto send historical data to new instances of a destination.
To connect a source to more than one instance of a destination in the Segment web app, start by adding the first instance of the destination and giving it a unique name,as described above. To add another instance of the destination, follow either of those two methods and choose another unique name.
You must give each instance of the destination connected to the same source a unique name. Segment recommends that you use descriptive names rather than numbers, so other Segment users can understand which Segment destinations are linked to which tool instances. For example, you might use “Amplitude North America” and “Amplitude South America”, instead of “Amplitude 1” and “Amplitude 2”. You can edit the destination instance name at any time.
If you added the first instance of your destination before multi-instance destinations became available, that instance is automatically named for the destination with no other identifiers, for example “Amplitude”.
Some destinations do not support having multiple instances connected to the same source. In that case, the option to add a second instance of that destination does not appear.
You can create unique destination filters for each destination instance connected to the same source.
Some destinations don’t support multiple instances connected to the same source. If this is the case, you won’t see the option to add a second instance of that destination.
It is not possible to connect multiple instances of one source (for example, two website sources) to the same destination. However, you can create another instance of the destination for the other sources, and clickCopy Settings From Other Destinationto save yourself time entering the settings values again.
You can add multiple instances of a destination using the Segment Public API. See the Segment ConfigAPI documentation. If a destination does not support multi-instance, the Public API throws an appropriate error.
Multiple Data Lakes:Segment does not currently support connecting a single source to multiple instances of a data lakes destination.Contact Segment Customer Successif this would be useful for your organization.
Protocols transformations and multi-instance support:Protocols transformations are specific to each source, and operate the same on all instances of a specific destination. Segment does not currently support creating unique protocols transformations for each instance of a destination.
Integrations object considerations:A common part of a Segment message isthe integrations object, which you can use to explicitly filter to which destinations the call is forwarded, as well as to specify options for different destination tools. If you use the integrations object to filter events or to send destination-specific options, Segment applies its values to all instances. For example:
In this example:

Headings:
Sending Segment Data to Destinations
Adding a destination
Adding a destination from the Segment web app
Adding a destination to a specific Segment Source
Adding a destination using the Public API
What happens when you add a destination
Analytics.js
Mobile sources
Server sources
Destination authentication
Destination settings
Connecting one source to multiple instances of a destination
Connect a source to more than one instance of a destination
Connect multiple sources to one instance of a destination
Connect to more than one instance of a destination using the Public API
Multi-instance destinations and Device-mode
Other multi-instance destination considerations
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/destinations/catalog/
Paragraphs:
Want a simpler list?
Check out thelist ofalldestinations.

Crawling: https://segment.com/docs/connections/destinations/destination-filters/
Paragraphs:
On this page
Destination filters are only available to Business Tier customers.
Use destination filters to prevent certain data from flowing into a destination. You can conditionally filter out event properties, traits, and fields, or even filter out the event itself.
You can configure destination filters on cloud-mode, mobile cloud-mode destinations, and web device-mode and actions-based destinations.  With device-mode destinations, you can use the same user interface or API mechanism that you use for your cloud-mode destinations, and have those filters acted upon for device-mode destinations on web.
Common use cases for destination filters include:
Keep the following limitations in mind when you use destination filters:
Contact Segmentif these limitations impact your use case.
To create a destination filter:
Enable destination filters for Analytics.js sources
If you are currently using Analytics.js as your source and want to apply filters to device-mode destinations, you need to enable device mode destination filters for your Analytics.js source. To do this, go to your Javascript source, navigate to Settings > Analytics.js, and turn on the toggle forDestination Filters. This will ensure the filters are effectively applied to device-mode destinations.
The destination filters API provides more power than Segment’s dashboard destination filters settings. With the API, you can create complex filters that are conditionally applied using Segment’sFilter Query Language (FQL).
The destination filters API offers four different filter types:
To learn more, read Segment’sDestination Filters API docs.
The following examples illustrate common destinations filters use cases:
Example: Remove email addresses fromcontextandproperties:
Property-level allowlisting is available with Segment’s API. Using destination filters, you can configure a rule that removes email addresses fromcontextandproperties. As a result, Segment only sends traits without PII to the destination.

Healthcare and Life Sciences (HLS) customers can encrypt data flowing into their destinations
HLS customers with a HIPAA eligible workspace can encrypt data in fields marked as Yellow in the Privacy Portal before they flow into an event stream, cloud-mode destination.
To learn more about data encryption, see theHIPAA Eligible Segment documentation.
This example shows a filter that controls event volume by only sendingUser Signed UpandDemo Requestedevents.

This example shows a rule that only sends track calls to Google Analytics.

In the example below, the rule targets email addresses with internal domains to stop test events from reaching Destinations.

In the example below, the rule prevents an event from sending ifOrder Completedandproperties.emailcontain an internal@segment.comemail address.

Using thedestination filters API, you can create a rule to randomly sample video heartbeat events.
Watch this destination filters walkthroughto learn how to use event names to filter events sent to destinations.
Use thePublic APIto only send events to your destination if they contain auserId. Here’s an example of how you might format this request:
There are certain destinations to which you may not want to send theuserId. To accomplish this, you can use thePublic APIto create a Filter that will target and remove theuserId(or any other top-level field) like this:
Some destinations offer settings that also allow you to filter data. For example, the Facebook App Events destination allows you to mapScreenevents toTrackevents. Because destination filters are evaluated and appliedbeforethe destination settings are applied, they can conflict with your settings.
For example, if you have a destination filter that filters Track eventsandyou have theUse Screen Events as Track Eventssetting enabled,Trackevents drop, butScreenevents still process. The destination settings transform it into aTrackevent -afterthe filters.
Segment makes effort to ensure that destination filters can handle unexpected situations. For example, if you use thecontains()FQL function on thenullfield, Segment returnsfalseinstead of returning an error. If Segment can’t infer your intent, Segment logs an internal error and drops the event. Segment defaults to this behavior to prevent sensitive information, like a PII filter, from getting through.
Errors aren’t exposed in your Destination’s Event Deliverability tab. For help diagnosing missing destination filter events,contact Segment.
Destination filters can filter properties out of objects nested in an array. For example, you can filter out thepriceproperty of every object in an array atproperties.products. You can also filter out an entire array from the payload. However, you can’t drop nested objects in an array or filter properties out of a single object in an array.
To block a specific property from all of the objects within a properties array, set the filter using the following the format:<propertyType>.<arrayName>.<arrayElementLabel>​.
For example, theproperties.products.newElementfilter blocks allnewElementproperty fields from eachproductsobject of an array within thepropertiesobject of a Track event.

To block the Identify event traitproducts.newElement, select the option under theUser Traitslist instead. To block the context object fieldproducts.newElement, select it from theContext Fieldslist.
Segment supports 10 filters per destination. If you need help consolidating filters or would like to discuss your use case,contact Segment.
Segment evaluates multipleOnly Sendfilters against each other and resolves destination filters in order. If multipleOnly Sendfilters conflict with each other, Segment won’t send information downstream.
Segment displays the most recent 15,000 properties. To find a property not in the filter dropdown, enter the property manually.
To filter out events from warehouses, use Selective Sync.
Generally, only Track calls havenameproperties, which correspond to theeventfield in an event.
The Activity Feed shows the action, date, and user who performed the action when a destination filter is created, modified, enabled, disabled, or deleted. You can also subscribe to notifications for any of these changes in theActivity Feedsettings page.
You must have write access to save and edit filters. Read permission access only allows viewing and testing access.
Use the destination filter tester during setup to verify that you’re filtering out the right events. Filtered events show up on the schema page but aren’t counted in event deliverability graphs.
Destination Filters can’t target properties or traits with spaces in the field name. As an alternative, useInsert Functions, which let you write code to take care of such filtering.
The check for unsupported events types happens before any destination filter checks. As a result, Destination Filters can’t prevent unsupported event type errors. To filter these events, use theIntegrations Object.
Destination filters only filter events sent after filter setup. If you just added a destination filter but still see some events going through, you’re likely seeing retries from failed events that occurred before you set up the filter.
When Segment sends an event to a destination but encounters a timeout error, it attempts to send the event again. As a result, if you add a destination filter while Segment is trying to send a failed event, these retries could filter through, since they reflect events that occurred before filter setup.
Destination filters are case-sensitive. Make sure to test your filter conditions with a test event before saving and enabling the filter.

Headings:
Destination Filters
Limitations
Create a destination filter
Destination filters API
Examples
PII management
Control event volume
Cleaner data
Remove internal and test events from production tools
Sample a percentage of events
Drop events
Only send events with userId
Remove userId from payload
Filter conditional operators
Important notes
Conflicting settings
Error handling
FAQs
How do destination filters work with array properties?
How many filters can I create?
Can I set multipleOnly Senddestination filters?
How many properties can I view in the filter dropdown?
How can I filter out warehouse events?
I don’t see anameproperty at the top level of my events to filter oneventname”.
How can I find out when new destination filters have been added or removed?
Why am I getting a permissions denied error when I try to save a filter?
How can I test my filter?
Can I filter on properties/traits that have spaces in the name (for example,properties.test event field)?
Can I use destination filters to drop events unsupported by a destination?
Why do I see events sent through after I just added a destination filter?
How do destination filters handle Protocols Transformations?
Are destination filter conditions case-sensitive?
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/reverse-etl/
Paragraphs:
Reverse ETL (Extract, Transform, Load) extracts data from a warehouse using a query you provide and syncs this warehouse data to your third party destinations.
Use Reverse ETL when you want to:
Reverse ETL supports event and object data
Event and object data includes customer profile data, subscriptions, product tables, shopping cart tables, and more.
Set up the infrastructure you need to sync data from your warehouse to your downstream destinations.
View your sync history, reset your syncs, or subscribe to alerts.
Learn more about the system that powers Reverse ETL, supported destinations, and frequently asked questions.
Reference material about system limits and how Segment detects data changes.
View the destinations you can connect to your Reverse ETL sources.
Frequently asked questions about Reverse ETL.
In this blog from Segment, learn how Reverse ETL helps businesses activate their data to drive better decision-making and greater operational efficiency.
Learn how MongoDB used Reverse ETL to connect the work of analytics teams to downstream marketing and sales tools to deliver just-in-time communications that increased customer satisfaction and engagement.

Headings:
Reverse ETL
Get started with Reverse ETL
Set up Reverse ETL
Manage Reverse ETL Syncs
Learn more
Reverse ETL System
Destination catalog
Reverse ETL FAQ
More Reverse ETL resources
What is Reverse ETL? A complete guide
Customer story: MongoDB
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/reverse-etl/setup/
Paragraphs:
On this page
There are four components to Reverse ETL: Sources, Models, Destinations, and Mappings.

Follow these 4 steps to set up Reverse ETL:
In Reverse ETL, a source is your data warehouse.
You need to be a user that has both read and write access to the warehouse.
To add your warehouse as a source:
Models define sets of data you want to sync to your Reverse ETL destinations. A source can have multiple models. Segment supportsSQL modelsanddbt models.
Use Segment’s dbt extension to centralize model management and versioning. Users who set up a BigQuery, Databricks, Postgres, Redshift, or Snowflake source can use Segment’sdbt extensionto centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.
If you use dbt Cloud with Reverse ETL, you cancreate up to 5 mappingsthat use the sync strategydbt Cloud, which extracts data from your warehouse and syncs it with your destination after a job in dbt Cloud is complete.
In Reverse ETL, destinations are the business tools or apps you use that Segment syncs the data from your warehouse to. A model can have multiple destinations.
Reverse ETL supports the destinations in theReverse ETL catalog. If the destination you want to send data to is not listed in the Reverse ETL catalog, use theSegment Connections Destinationto send data from your Reverse ETL warehouse to your destination.
Engage users can use theSegment Profiles Destinationto create and updateProfilesthat can then be accessed throughProfile APIand activated withinTwilio Engage.
Separate endpoints and credentials required to set up third party destinations
Before you begin setting up your destinations, note that each destination has different authentication requirements. See the documentation for your intended destination for more details.
To add your first destination:
Mappings enable you to map the data you extract from your warehouse to the fields in your destination. A destination can have multiple mappings.
To create a mapping:
Use Segment’sDuplicate mappingsfeature to create an exact copy of an existing mapping. The copied mapping has the same configurations and enrichments as your original mapping.
When you set up destination actions in Reverse ETL, depending on the destination, somemapping fieldsmay require data as anobjectorarray.
You can send data to a mapping field that requires object data. An example of object mapping is anOrder completedmodel with aProductscolumn that’s in object format.
Example:
To send data to a mapping field that requires object data, you can choose between these two options:
Certain object mapping fields have a fixed list of properties they can accept. If the names of the nested properties in your object don’t match with the destination properties, the data won’t send. Segment recommends you to useCustomize Objectto ensure your mapping is successful.
To send data to a mapping field that requires array data, the model must provide data in the format of an array of objects. An example is anOrder completedmodel with aProduct purchasedcolumn that’s in an array format.
Example:
To send data to a mapping field that requires array data, you can choose between these two options:
Certain array mapping fields have a fixed list of properties they can accept. If the names of the nested properties in your array don’t match the destination properties, the data won’t send. Segment recommends you to use theCustomize arrayoption to ensure your mapping is successful.
Objects in an array don’t need to have the same properties. If a user selects a missing property in the input object for a mapping field, the output object will miss the property.
You can choose to exclude null values from optional mapping fields in your syncs to some destinations. Excluding null values helps you maintain data integrity in your downstream destinations, as syncing a null value for an optional field may overwrite an existing value in your downstream tool.
For example, if you opt to sync null values with your destination and an end user fills out a form but chooses to leave an optional telephone number field blank, the existing telephone number you have on file in your destination could be overwritten with the null value. By opting out of null values for your downstream destination, you would preserve the existing telephone number in your destination.
By default, Segment syncs null values from mapped fields to your downstream destinations. Some destinations do not allow the syncing of null values, and will reject requests that contain them. Segment disables the option to opt out of syncing null values for these destinations.
To opt out of including null values in your downstream syncs:
After you’ve set up your source, model, destination, and mappings for Reverse ETL, your data will extract and sync to your destination(s) right away if you chose an interval schedule. If you set your data to extract at a specific day and time, the extraction will take place then.
To edit your model:
Suggested mappings is fully available for RETL mappings.
Segment offers suggested mappings that automatically propose relevant destination fields for model columns and payload elements. For example, if your model includes a column or payload field namedtransaction_amount, the feature might suggest mapping it to a destination field likeAmountorTransactionValue. This automation, powered by intelligent autocompletion, matches and identifies near-matching field names to streamline the mappings setup process. For more information, seeSegment’s suggested mappings blog postand theSuggested Mappings Nutrition Facts Label.
Review the suggested mappings for accuracy before finalizing them, as Segment can’t guarantee all of the suggested mappings are accurate.
To edit your mapping:

Headings:
Set up Reverse ETL
Step 1: Add a source
Step 2: Add a model
SQL editor
dbt model
Step 3: Add a destination
Step 4: Create mappings
Supported object and arrays
Object mapping
Array mapping
Null value management
Initial sync for a given mapping
Edit Reverse ETL syncs
Edit your model
Suggested mappings
Edit your mapping
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/reverse-etl/manage-retl/
Paragraphs:
On this page
View your sync history, reset your syncs, or subscribe to alerts.
The Reverse ETL sync overview tab, located underConnections > Destinations, gives you an overview of your latest Reverse ETL syncs.

You can view the following information about each sync:
Check the status of your data extractions and see details of your syncs. Click into failed records to view additional details on the error, sample payloads to help you debug the issue, and recommended actions.
To check the status of your extractions:
Automatic retry handling might not yet be available in your workspace
To ensure overall system stability and performance, Segment is releasing automatic retry handling to all workspaces in a phased rollout program. Segment expects this feature to be available to all customers by January 31, 2025.
Segment automatically retries events that were extracted from your data warehouse but failed to load for up to 14 days or 5 syncs following a partially successful sync or a sync failure.
Segment checks for the latest changes in your data before loading the failed records on a subsequent (automatically scheduled or manually triggered) sync to ensure the data loaded into Segment isn’t stale and only the latest version of the data is loaded to destination. If the error causing the load failure is coming from an upstream tool, you can fix the error in the upstream tool to resolve the load error on a subsequent sync.
Syncs with intervals less than or equal to two hours may not see failed events on the sync immediately following failed record
Syncs with intervals less than or equal to two hours may not see failed events right away, as Segment’s internal systems take up to two hours to retry events that initially failed.
Reverse ETL uses the Unique Identifier column to detect data changes, like new, updated, and deleted records. If you encounter an error, you can reset Segment’s tracking of this column and force Segment to manually add all records from your dataset.
To reset a sync:
You can cancel syncs when your sync is currently running during the extraction and load phase.
To cancel a sync:
Your canceled syncs with have a status asCanceled,and any syncs that are in the process of being canceled will have a status ofCanceling.
Once you cancel a sync, the record count underExtraction Resultsreflects the records already processed. These records won’t be included in future syncs. To reprocess these records, you can reset or replay the sync.
You can choose to replay syncs. To replay a specific sync, contactfriends@segment.com. Keep in mind that triggering a replay resyncs all records for a given sync.
You can opt in to receive email, Slack, and in-app alerts about Reverse ETL sync failures and fluctuations in the volume of events successfully delivered to your mapping.
The notification channels that you select for one alert will apply to all alerts in your workspace.
To subscribe to alerts for a failed or partially successful sync:
If you opted to receive notifications by email, you can clickView active email addressesto see the email addresses that are currently signed up to receive notifications.
You can create an alert that notifies you when the volume of events successfully received by your mapping in the last 24 hours falls below a percentage you set. For example, if you set a percentage of 99%, Segment notifies you if your destination had a successful delivery rate of 98% or below.
To receive a successful delivery rate fluctuation alert in a Slack channel, you must first create a Slack webhook. For more information about Slack webhooks, see Slack’sSending messages using incoming webhooksdocumentation.

To subscribe to alerts for successful delivery fluctuations at the mapping level:
To edit or disable your alert, navigate to your mapping’s Alerts tab and select the Actions menu for the alert you’d like to edit.
This page was last modified: 12 Dec 2024

Headings:
Manage Reverse ETL Syncs
Sync overview
Sync history
Automatic retry handling
Reset syncs
Cancel syncs
Replays
Alerting
Failed or partially successful syncs
Mapping-level successful delivery rate fluctuations
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/reverse-etl/system/
Paragraphs:
On this page
View reference information about how Segment detects data changes in your warehouse and the rate and usage limits associated with Reverse ETL.
Reverse ETL computes the incremental changes to your data directly within your data warehouse. The Unique Identifier column is used to detect the data changes, such as new, updated, and deleted records.
Delete Records Payload
The only value passed for deleted records is their unique ID, which can be accessed as__segment_id. As of September 24, 2024, deleted records also contain all columns selected by your model, withnullvalues in place of data.
For Segment to compute the data changes within your warehouse, Segment needs to have both read and write permissions to the warehouse schema table. At a high level, the extract process requires read permissions for the query being executed. Segment keeps track of changes to the query results through tables that Segment manages in a dedicated schema (for example,_segment_reverse_etl), which requires some write permissions.
There may be cost implications to having Segment query your warehouse tables.
To provide consistent performance and reliability at scale, Segment enforces default use and rate limits for Reverse ETL.
Reverse ETL usage limits are measured based on the number of records processed to each destination – this includes both successful and failed records. For example, if you processed 50K records to Braze and 50K records to Mixpanel, then your total Reverse ETL usage is 100K records.
Processed records represents the number of records Segment attempts to send to each destination. Keep in mind that not all processed records are successfully delivered, for example, such as when the destination experiences an issue.
Your plan determines how many Reverse ETL records you can process in one monthly billing cycle. When your limit is reached before the end of your billing period, your syncs will pause and then resume on your next billing cycle. To see how many records you’ve processed using Reverse ETL, navigate toSettings > Usage & billingand select theReverse ETLtab.
If you have a non-standard or high volume usage plan, you may have unique Reverse ETL limits or custom pricing. To see your Reverse ETL limits in the Segment app, selectSettings > Usage & Billing.
The extract phase is the time spent connecting to your database, executing the model query, updating internal state tables and staging the extracted records for loading.
*:If Segment identifies a sync would be larger than 150 million records, Segment extracts 150 million of the records in the initial sync and syncs any additional records during the next scheduled or manual sync.
For example, if a sync would contain 700 million records, Segment would run an initial 150 million record sync, and during the next three scheduled or manual syncs, would sync 150 million records. The fifth scheduled or manual sync would contain the remaining 100 million records.
This page was last modified: 25 Sep 2024
Questions? Problems? Need more info? Contact Segment Support for assistance!
Thanks for your feedback!
Can we improve this doc?Send us feedback!
On this page
Was this page helpful?
Thanks for your feedback!
Can we improvethis doc?Send us feedback!
Product
For Developers
Company
Support
© 2025 Segment.io, Inc.

Headings:
Reverse ETL System
Record diffing
Limits
Usage limits
Configuration limits
Extract limits
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/reverse-etl/reverse-etl-catalog/
Paragraphs:
On this page
Reverse ETL supports the Actions destinations listed in this catalog. Most destinations not listed here are supported through theSegment Connectionsdestination.
Twilio Engage Premier Subscriptions users can use theSegment Profilesdestination to enrich their warehouse data.
The following destinations natively supportReverse ETL. If you don’t see your destination listed in the Reverse ETL catalog, use theSegment Connections destinationto send data from your Reverse ETL warehouse to other destinations listed in thecatalog.

If you don’t see your destination listed in the Reverse ETL catalog, use theSegment Connections destinationto send data from your Reverse ETL warehouse to other destinations listed in thecatalog.
The Segment Connections destination enables you to mold data extracted from your warehouse inSegment SpecAPI calls that are then processed bySegment’s HTTP Tracking API. The requests hit Segment’s servers, and then Segment routes your data to any destination you want. Get started with theSegment Connections destination.
The Segment Connections destination sends data to Segment’s Tracking API, which has cost implications. New users count as new MTUs and each call counts as an API call. For information on how Segment calculates MTUs and API calls, please seeMTUs, Throughput and Billing.
Engage Premier Subscriptions users can use Reverse ETL to sync subscription data from warehouses to destinations.
To get started with using Reverse ETL for subscripti

Headings:
Reverse ETL Catalog
Actable Predictive
Actions Pipedrive
Adobe Target Cloud Mode
Aggregations.io (Actions)
Airship (Actions)
Algolia Insights (Actions)
Amplitude (Actions)
AppFit
Attio (Actions)
Avo
Blackbaud Raiser's Edge NXT
Blend Ai
Braze Cloud Mode (Actions)
Braze Cohorts
Canny (Actions)
CleverTap (Actions)
Close
Cordial (Actions)
Criteo Audiences
Customer.io (Actions)
Emarsys (Actions)
Encharge (Actions)
Equals
Facebook Conversions API (Actions)
Facebook Custom Audiences (Actions)
Friendbuy (Cloud Destination)
Fullstory Cloud Mode (Actions)
Gainsight Px Cloud (Actions)
Gleap (Action)
Google Ads Conversions
Google Analytics 4 Cloud
Google Sheets
GWEN (Actions)
HubSpot Cloud Mode (Actions)
Insider Audiences
Insider Cloud Mode (Actions)
Intercom Cloud Mode (Actions)
Iterable (Actions)
June (Actions)
Kafka
Klaviyo (Actions)
Koala (Cloud)
LaunchDarkly (Actions)
LinkedIn Audiences
Listrak (Actions)
LiveLike
Loops (Actions)
Marketo Static Lists (Actions)
Metronome (Actions)
Mixpanel (Actions)
Moengage (Actions)
Movable Ink (Actions)
Optimizely Feature Experimentation (Actions)
Pardot (Actions)
Pinterest Conversions API
Podscribe (Actions)
Pushwoosh
Qualtrics
Rehook
RevX Cloud (Actions)
Ripe Cloud Mode (Actions)
Salesforce (Actions)
Salesforce Marketing Cloud (Actions)
Saleswings (Actions)
Segment Connections
Segment Profiles
SendGrid
Slack (Actions)
Snapchat Conversions API
StackAdapt
Talon.One (Actions)
TikTok Conversions
Tiktok Offline Conversions
Toplyne Cloud Mode (Actions)
Userpilot Cloud (Actions)
Voucherify (Actions)
VWO Cloud Mode (Actions)
Webhooks (Actions)
Segment Connections destination
Send data to Engage with Segment Profiles
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/reverse-etl/reverse-etl-source-setup-guides/azure-setup/
Paragraphs:
On this page
Set up Azure as your Reverse ETL source.
At a high level, when you set up Azure dedicated SQL pools for Reverse ETL, the configured user needs read permissions for any resources (databases, schemas, tables) the query needs to access. Segment keeps track of changes to your query results with a managed schema (__SEGMENT_REVERSE_ETL), which requires the configured user to allow write permissions for that schema.
Make sure the user you use to connect to Segment has permissions to use that warehouse. You can follow the process below to set up a new user with sufficient permissions for Segment’s use.
To create a login in your master database, run:
Execute the commands below in the database where your data resides.
To create a user for Segment, run:
To grant access to the user to read data from all schemas in the database, run:
To grant Segment access to read from certain schemas, run:
To grant Segment access to create a schema to keep track of the running syncs, run:
If you want to create the schema yourself and then give Segment access to it, run:
To set up Azure as your Reverse ETL source:
After you’ve successfully added your Azure source,add a modeland follow the rest of the steps in the Reverse ETL setup guide.

Headings:
Azure Reverse ETL Setup
Required permissions
Set up guide
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/reverse-etl/reverse-etl-source-setup-guides/bigquery-setup/
Paragraphs:
On this page
To set up your BigQuery source with Reverse ETL, you mustconstruct a BigQuery role and service accountandcreate a BigQuery source in the Segment app.
BigQuery Reverse ETL sources support Segment's dbt extension
If you have an existing dbt account with a Git repository, you can useSegment’s dbt extensionto centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.
You need to be an account admin to set up the Segment BigQuery connector as well as write permissions for the__segment_reverse_etldataset.
There are two approaches you can take when granting Segment access to your BigQuery resources:
You can choose the approach that best suits your needs.
With this approach, use BigQuery predefined roles to create a service account for Segment to assume.
Enter yourService account nameand a description of what the service account will do.
With this approach, you can set up a custom role with the following permissions:
When connecting your BigQuery warehouse to Segment, you’ll need to know the location of your resources.
You can find the location of your BigQuery resources using the following method:
After you’ve added BigQuery as a source, you canadd a modeland follow the rest of the steps in the Reverse ETL setup guide.

Headings:
BigQuery Reverse ETL Setup
Constructing your own role or policy
Grant Full Access
Grant Limited Access
BigQuery resource location
Set up BigQuery as your Reverse ETL source
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/reverse-etl/reverse-etl-source-setup-guides/databricks-setup/
Paragraphs:
On this page
Set up Databricks as your Reverse ETL source.
At a high level, when you set up Databricks for Reverse ETL, the configured service-principal needs read permissions for any resources (databases, schemas, tables) the query needs to access. Segment keeps track of changes to your query results with a managed schema (__SEGMENT_REVERSE_ETL), which requires the configured service-principal to allow write permissions for that schema. Segment supports only OAuth (M2M) authentication. To generate a client ID and Secret, follow the steps listed in Databricks’OAuth machine-to-machine (M2M) authenticationdocumentation.
Databricks Reverse ETL sources support Segment's dbt extension
If you have an existing dbt account with a Git repository, you can useSegment’s dbt extensionto centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.
Make sure the service principal you use to connect to Segment has permissions to use that warehouse. In the Databricks console go toSQL warehousesand select the warehouse you’re using. Navigate toOverview > Permissionsand make sure the service principal you use to connect to Segment hascan usepermissions.
To grant access to read data from the tables used in the model query, run:
To grant Segment access to create a schema to keep track of the running syncs, run:
If you want to create the schema yourself instead and then give Segment access to it, run:
To set up Databricks as your Reverse ETL source:
Segment previously supported token-based authentication, but now uses OAuth (M2M) authentication at the recommendation of Databricks.
If you previously set up your source using token-based authentication, Segment will continue to support it. If you want to create a new source or update the connection settings of an existing source, Segment only supportsOAuth machine-to-machine (M2M) authentication.
After you’ve successfully added your Databricks source,add a modeland follow the rest of the steps in the Reverse ETL setup guide.

Headings:
Databricks Reverse ETL Setup
Required permissions
Set up guide
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/reverse-etl/reverse-etl-source-setup-guides/postgres-setup/
Paragraphs:
On this page
Set up Postgres as your Reverse ETL source.
At a high level, when you set up Postgres for Reverse ETL, the configured user/role needs read permissions for any resources (databases, schemas, tables) the query needs to access. Segment keeps track of changes to your query results with a managed schema (__SEGMENT_REVERSE_ETL), which requires the configured user to allow write permissions for that schema.
Postgres Reverse ETL sources support Segment's dbt extension
If you have an existing dbt account with a Git repository, you can useSegment’s dbt extensionto centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.
Segment supports the following Postgres database providers:
Segment only supports these Postgres database providers. Postgres databases from other providers aren’t guaranteed to work. For questions or concerns about Segment-supported Postgres providers,contact Segment Support.
To set up Postgres with Reverse ETL:
Run the SQL commands below to create a user namedsegment.
Give thesegmentuser read permissions for any resources (databases, schemas, tables) the query needs to access.
Give thesegmentuser write permissions for the Segment managed schema (__SEGMENT_REVERSE_ETL), which keeps track of changes to the query results.
After you’ve successfully added your Postgres source,add a modeland follow the rest of the steps in the Reverse ETL setup guide.

Headings:
Postgres Reverse ETL Setup
Set up guide
Extra permissions
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/reverse-etl/reverse-etl-source-setup-guides/redshift-setup/
Paragraphs:
On this page
Set up Redshift as your Reverse ETL source.
Redshift Reverse ETL sources support Segment's dbt extension
If you have an existing dbt account with a Git repository, you can useSegment’s dbt extensionto centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.
To set up Redshift with Reverse ETL:
Run the SQL commands below to create a user namedsegment.
Give thesegmentuser read permissions for any resources (databases, schemas, tables) the query needs to access.
Give thesegmentuser write permissions for the Segment managed schema (__segment_reverse_etl), which keeps track of changes to the query results.
If you are able to run the query in the Query Builder, but the sync fails with therelation does not existerror, please make sure the schema name is included before the database table name, and check that the schema name is correct:
After you’ve successfully added your Redshift source,add a modeland follow the rest of the steps in the Reverse ETL setup guide.

Headings:
Redshift Reverse ETL Setup
Extra Permissions
Troubleshooting
Extraction failures: relation does not exist
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/reverse-etl/reverse-etl-source-setup-guides/snowflake-setup/
Paragraphs:
On this page
Set up Snowflake as your Reverse ETL source.
At a high level, when you set up Snowflake for Reverse ETL, the configured user/role needs read permissions for any resources (databases, schemas, tables) the query needs to access. Segment keeps track of changes to your query results with a managed schema(__SEGMENT_REVERSE_ETL), which requires the configured user to allow write permissions for that schema.
Segment now supports key-pair authentication for Snowflake Reverse ETL sources. Key-pair authentication is available for Business Tier users only.
Snowflake Reverse ETL sources support Segment's dbt extension
If you have an existing dbt account with a Git repository, you can useSegment’s dbt extensionto centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.
Follow the instructions below to set up the Segment Snowflake connector. Segment recommends you use theACCOUNTADMINrole to execute all the commands below, and that you create a user that authenticates with an encrypted key pair.
Segment has a Terraform provider, powered by the Public API, that you can use to create a Snowflake Reverse ETL source. See thesegment_source (Resource)documentation for more information.
Enter and run the code below to create a database.
Segment uses the database specified in your connection settings to create a schema called__segment_reverse_etlto avoid collision with your data. The schema is used for tracking changes to your model query results between syncs.
An existing database can be reused, if desired. Segment recommends you to use the same database across all your models attached to this source to keep all the state tracking tables in 1 place.
Enter and run the code below to create a virtual warehouse.
Segment Reverse ETL needs to execute queries on your Snowflake account, which requires a Virtual Warehouse to handle the compute. You can also reuse an existing warehouse.
Enter and run the code below to create specific roles for Reverse ETL.
All Snowflake access is specified through roles, which are then assigned to the user you’ll create later.
Enter and run one of the following code snippets below to create the user Segment uses to run queries. For added security, Segment recommends creating a user that authenticates using a key pair.
To create a user that authenticates with a key pair,create a key pairand then execute the following SQL commands:
To create a user that authenticates with a password, execute the following SQL commands:
Learn more about the Snowflake Account ID in Snowflake’sAccount identifiersdocumentation.
After you’ve successfully added your Snowflake source,add a modeland follow the rest of the steps in the Reverse ETL setup guide.
If you create a network policy with Snowflake and are located in the US, add52.25.130.38/32and34.223.203.0/28to the “Allowed IP Addresses” list.
If you create a network policy with Snowflake and are located in the EU, add3.251.148.96/29to your “Allowed IP Addresses” list.

Headings:
Snowflake Reverse ETL Setup
Set up guide
Security
Allowlisting IPs
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/functions/
Paragraphs:
On this page
Functions let you create your own sources and destinations directly within your workspace to bring new types of data into Segment and send data to new tools with just a few lines of JavaScript - no additional infrastructure required.
Functions is available to all customer plan types with a free allotment of usage hours. Read more aboutFunctions usage limits, or seeyour workspace’s Functions usage stats.

Functions can help you bring external data into Segment (Source functions) and send data in Segment out to external destinations (Destination functions). UseInsert functionsto transform data before it reaches your downstream destinations. Functions are scoped to your specific workspace. If you’re a technology partner and want to build a new integration and publish it in Segment’s catalog, see theDeveloper Center documentation.
Source functions receive external data from a webhook and can create Segment events, objects, or both. Source functions have access to the full power of JavaScript so you can validate and transform the incoming data and even make external API requests to annotate your data.
Use cases:
Learn more aboutsource functions.
Destination functions can take events from a Segment source, transform the events, and deliver them to external APIs. Destination functions can make arbitrary requests to annotate data, as well.
Use cases:
Learn more aboutdestination functions.
Destination insert functions help you enrich your data with code before you send it to downstream destinations.
Use cases:
To learn more, visitdestination insert functions.
With Functions Copilot, you can instrument custom integrations, enrich and transform data, and even secure sensitive data nearly instantaneously without writing a line of code.
To learn more, visit theFunctions Copilot documentation.
IP Allowlisting uses a NAT gateway to route outbound Functions traffic from Segment’s servers to your destinations through a limited range of IP addresses, which can prevent malicious actors from establishing TCP and UDP connections with your integrations.
IP Allowlisting is available for customers on Business Tier plans.
To learn more, visitSegment’s IP Allowlisting documentation.

Headings:
Functions Overview
What can you do with Functions?
Source functions
Destination functions
Destination insert functions
Functions Copilot
IP Allowlisting
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/functions/source-functions/
Paragraphs:
On this page
Source functions allow you to gather data from any third-party applications without worrying about setting up or maintaining any infrastructure.
All functions are scoped to your workspace, so members of other workspaces cannot view or use them.
Functions is available to all customer plan types with a free allotment of usage hours. Read more aboutFunctions usage limits, or seeyour workspace’s Functions usage stats.

After you clickBuild, a code editor appears. Use the editor to write the code for your function, configure settings, and test the function’s behavior.
Tip:Want to see some example functions? Check out the templates available in the Functions UI, or in the open-sourceSegment Functions Library. (Contributions welcome!)

Source functions must have anonRequest()function defined.
This function is executed by Segment for each HTTPS request sent to this function’s webhook.
TheonRequest()function receives two arguments:
To parse the JSON body of the request, use therequest.json()method, as in the following example:
Use therequest.headersobject to get values of request headers.
Since it’s an instance ofHeaders, the API is the same in both the browser and in Node.js.
To access the URL details, refer torequest.urlobject, which is an instance ofURL.
You can send messages to the Segment API using theSegmentobject:
UseIdentify callsto connect users with their actions, and to record traits about them.
TheSegment.identify()method accepts an object with the following fields:
Track callsrecord actions that users perform, along with any properties that describe the action.
TheSegment.track()method accepts an object with the following fields:
Group callsassociate users with a group, like a company, organization, account, project, or team.
TheSegment.group()method accepts an object with the following fields:
Page callsrecord whenever a user sees a page of your website, along with any other properties about the page.
TheSegment.page()method accepts an object with the following fields:
Screen callsrecord when a user sees a screen, the mobile equivalent ofPage, in your mobile app.
TheSegment.screen()method accepts an object with the following fields:
TheAlias callmerges two user identities, effectively connecting two sets of user data as one.
TheSegment.alias()method accepts an object with the following fields:
The Set call usesthe object APIto save object data to your Redshift, BigQuery, Snowflake, or other data warehouses supported by Segment.
TheSegment.set()method accepts an object with the following fields:
When you use theset()method, you won’t see events in the Source Debugger. Segment only sends events to connected warehouses.
On March 26, 2024, Segment is upgrading the Functions runtime environment to Node.js v18, which is the current long-term support (LTS) release.
This upgrade keeps your runtime current with industry standards. Based on theAWS LambdaandNode.jssupport schedule, Node.js v16 is no longer inMaintenance LTS. Production applications should only use releases of Node.js that are inActive LTSorMaintenance LTS.
All new functions will use Node.js v18 starting March 26, 2024.
For existing functions, this change automatically occurs as you update and deploy an existing function. Segment recommends that you check your function post-deployment to ensure everything’s working. Your function may face issues due to the change in sytax between different Node.js versions and dependency compatibility.
Limited time opt-out option
If you need more time to prepare, you can opt out of the update before March 19, 2024.Note that if you opt out:- The existing functions will continue working on Node.js v16.- You won’t be able to create new functions after July 15, 2024.- You won’t be able to update existing functions after August 15, 2024.- You won’t receive future bug fixes, enhancements, and dependency updates to the functions runtime.Contact Segmentto opt-out or with any questions.
Node.js 18
Segment strongly recommends updating to Node.js v18 to benefit from future runtime updates, the latest security, and performance improvements.
Functions do not currently support importing dependencies, but you cancontact Segment Supportto request that one be added.
The following dependencies are installed in the function environment by default.
zlib v1.0.5exposed aszlib.zlib
uuidv5is exposed as an object. Useuuidv5.uuidv5to access its functions. For example:
zlib’s asynchronous methodsinflateanddeflatemust be used withasyncorawait. For example:
The following Node.js modules are available:
Other built-in Node.js modulesaren’t available.
For more information on using theaws-sdkmodule, see how toset up functions for calling AWS APIs.
Basic cache storage is available through thecacheobject, which has the following methods defined:
Some important notes about the cache:
Settings allow you to pass configurable variables to your function, which is the best way to pass sensitive information such as security tokens. For example, you might usesettingsas placeholders to use information such as an API endpoint and API key. This way, you can use the same code with different settings for different purposes. When you deploy a function in your workspace, you are prompted to fill out these settings to configure the function.
First, add a setting inSettingstab in the code editor:

ClickAdd Settingto add your new setting.

You can configure the details about this setting, which change how it’s displayed to anyone using your function:
As you change the values, a preview to the right updates to show how your setting will look and work.
ClickAdd Settingto save the new setting.
Once you save a setting, it appears in theSettingstab for the function. You can edit or delete settings from this tab.

Next, fill out this setting’s value inTesttab, so that you can run the function and check the setting values being passed.
Note, this value is only for testing your function.

Now that you’ve configured a setting and filled in a test value, you can add code to read its value and run the function:
When you deploy a source function in your workspace, you are prompted to fill out settings to configure the source. You can access these settings later by navigating to the Source Settings page for the source function.

You can test your code directly from the editor in two ways: either by receiving real HTTPS requests through a webhook, or by manually constructing an HTTPS request from within the editor.
The advantage of testing your source function with webhooks is that all incoming data is real, so you can test behavior while closely mimicking the production conditions.
Note: Segment has updated the webhook URL toapi.segmentapis.com/functions. To use webhooks with your function, you must:
You can use webhooks to test the source function either by sending requests manually (using any HTTP client such as cURL, Postman, or Insomnia), or by pasting the webhook into an external server that supports webhooks (such as Slack).A common Segment use case is to connect a Segmentwebhooks destinationorwebhook actions destinationto a test source, where the Webhook URL/endpoint that is used corresponds to the provided source function’s endpoint, then you can trigger test events to send directly to that source, which are routed through your Webhook destination and continue on to the source function: Source → Webhook destination → Source Function.
From the source function editor, copy the provided webhook URL (endpoint) from the “Auto-fill via Webhook” dialog.Note: When a new source is created that utilizes a source function, the new source’s endpoint (webhook URL) will differ from the URL that is provided in the source function’s test environment.
To test the source function:
You can also manually construct the headers and body of an HTTPS request inside the editor and test with this data without using webhooks.
TheContent-TypeHeader is required when testing the function:

After you finish building your source function, clickConfigureto name it, then clickCreate Functionto save it.
The source function appears on theFunctionspage in your workspace’s catalog.
If you’re editing an existing function, you canSavechanges without updating instances of the function that are already deployed and running.
You can also choose toSave & Deployto save the changes, and then choose which already-deployed functions to update with your changes. You might needadditional permissionsto update existing functions.
Your function may encounter errors that you missed during testing, or you might intentionally throw errors in your code (for example, if the incoming request is missing required fields).
If your function throws an error, execution halts immediately. Segment captures the incoming request, any console logs the function printed, and the error, and displays this information in the function’sErrorstab. You can use this tab to find and fix unexpected errors.

Functions can throwan Error or custom Error, and you can also add additional helpful context in logs using theconsoleAPI.
For example:
Warning:Do not log sensitive data, such as personally-identifying information (PII), authentication tokens, or other secrets. You should especially avoid logging entire request/response payloads. Segment only retains the 100 most recent errors and logs for up to 30 days but theErrorstab may be visible to other workspace members if they have the necessary permissions.
Segment only attempts to run your source function again if aRetryerror occurs.
Functions have specific roles which can be used foraccess managementin your Segment workspace.
Access to functions is controlled by two permissionsroles:
You also need additionalSource Adminpermissions to enable source functions, connect destination functions to a source, or to deploy changes to existing functions.
If you are aWorkspace OwnerorFunctions Admin, you can manage your source function from theFunctionstab in the catalog.
You must be aWorkspace OwnerorSource Adminto connect an instance of your function in your workspace.
From theFunctions tab, clickConnect Sourceand follow the prompts to set it up in your workspace.
After configuring, find the webhook URL - either on theOvervieworSettings → Endpointpage.
Copy and paste this URL into the upstream tool or service to send data to this source.
Segment retries invocations that throw RetryError or Timeout errors up to six times. After six attempts, the request is dropped.
The initial wait time for the retried event is a random value between one and three minutes.
Wait time increases exponentially after every retry attempt. The maximum wait time between attempts can reach 20 minutes.
Retry errors only appear in the source function error logs if the event has exhausted all six retry attempts and, as a result, has been dropped.
The maximum payload size for an incoming webhook payload is 512 KiB.
The execution time limit is five seconds, however Segment strongly recommends that you keep execution time as low as possible. If you are making multiple external requests you can use async / await to make them concurrently, which will help keep your execution time low.
Segment alphabetizes payload fields that come in todeployedsource functions. Segment doesn’t alphabetize payloads in the Functions tester. If you need to verify the exact payload that hits a source function, alphabetize it first. You can then make sure it matches what the source function ingests.
GETrequests are not supported with a source function. Source functions can only receive data throughPOSTrequests.
No. Tracking Pixels operate client-side only and need to be loaded onto your website directly. Source Functions operate server-side only, and aren’t able to capture or implement client-side tracking code. If the tool you’re hoping to integrate is server-side, then you can use a Source Function to connect it to Segment.
The test function interface has a 4KB console logging limit. Outputs surpassing this limit will not be visible in the user interface.
No, Source Functions can’t send custom responses to the tool that triggered the Function’s webhook. Source Functions can only send a success or failure response, not a custom one.
This error occurs because Segment prevents Source Functions from sending data back to their own webhook endpoint (https://fn.segmentapis.com). Allowing this could create an infinite loop where the function continuously triggers itself.
To resolve this error, check your Function code and ensure the URLhttps://fn.segmentapis.comis not included. This URL is used to send data to a Source Function and shouldn’t appear in your outgoing requests. Once you remove this URL from your code, you’ll be able to save the Function successfully.

Headings:
Source Functions
Create a source function
Code the source function
Request processing
Sending messages
Identify
Track
Group
Page
Screen
Alias
Set
Runtime and dependencies
Caching
Create settings and secrets
Test the source function
Testing source functions with a webhook
Testing source functions manually
Save and deploy the function
Source functions logs and errors
Error types
Managing source functions
Source functions permissions
Editing and deleting source functions
Connecting source functions
Source function FAQs
What is the retry policy for a webhook payload?
I configured RetryError in a function, but it doesn’t appear in my source function error log.
What is the maximum payload size for the incoming webhook?
What is the timeout for a function to execute?
Does Segment alter incoming payloads?
Does the source function allowGETrequests?
Can I use a Source Function in place of adding a Tracking Pixel to my code?
What is the maximum data size that can be displayed in console.logs() when testing a Function?
Can I send a custom response from my Source Function to an external tool?
Why am I seeing the error “Functions are unable to send data or events back to their originating source” when trying to save my Source Function?
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/functions/destination-functions/
Paragraphs:
On this page
Destination functions allow you to transform and annotate your Segment events and send them to any external tool or API without worrying about setting up or maintaining any infrastructure.
All functions are scoped to your workspace, so members of other workspaces can’t view or use them.
Functions is available to all customer plan types with a free allotment of usage hours. Read more aboutFunctions usage limits, or seeyour workspace’s Functions usage stats.

Destination functions doesn’t accept data fromObject Cloud sources. Destination functions don’t supportIP Allowlisting.
After you clickBuild, a code editor appears. Use the editor to write the code for your function, configure settings, and test the function’s behavior.
Tip:Want to see some example functions? Check out the templates available in the Functions UI, or in the open-sourceSegment Functions Library. (Contributions welcome!)
Segment invokes a separate part of the function (called a “handler”) for each event type that you send to your destination function.
Your function isn’t invoked for an event if you’ve configured adestination filter, and the event doesn’t pass the filter.
The default source code template includes handlers for all event types. You don’t need to implement all of them - just use the ones you need, and skip the ones you don’t.
Destination functions can define handlers for each message type in theSegment spec:
Each of the functions above accepts two arguments:
The example below shows a destination function that listens for “Track” events, and sends some details about them to an external service.
To change which event type the handler listens to, you can rename it to the name of the message type. For example, if you rename this functiononIdentify, it listens for “Identify” events instead.
Functions’ runtime includes afetch()polyfill using anode-fetchpackage. Check out thenode-fetch documentationfor usage examples.
Segment considers a function’s execution successful if it finishes without error. You can alsothrowan error to create a failure on purpose. Use these errors to validate event data before processing it, to ensure the function works as expected.
You canthrowthe following pre-defined error types to indicate that the function ran as expected, but that data was not deliverable:
The examples show basic uses of these error types.
If you don’t supply a function for an event type, Segment throws anEventNotSupportederror by default.
You can incorporate a atry-catchblock to ensure smooth operation of functions even when fetch calls fail. This allows for the interception of any errors during the API call, enabling the application of specific error handling procedures, such as error logging for future debugging, or the assignment of fallback values when the API call is unsuccessful. By positioning the continuation logic either outside thetry-catchblock or within afinallyblock, the function is guaranteed to proceed with its execution, maintaining its workflow irrespective of the outcome of the API call.
You can read more abouterror handlingbelow.
On March 26, 2024, Segment is upgrading the Functions runtime environment to Node.js v18, which is the current long-term support (LTS) release.
This upgrade keeps your runtime current with industry standards. Based on theAWS LambdaandNode.jssupport schedule, Node.js v16 is no longer inMaintenance LTS. Production applications should only use releases of Node.js that are inActive LTSorMaintenance LTS.
All new functions will use Node.js v18 starting March 26, 2024.
For existing functions, this change automatically occurs as you update and deploy an existing function. Segment recommends that you check your function post-deployment to ensure everything’s working. Your function may face issues due to the change in sytax between different Node.js versions and dependency compatibility.
Limited time opt-out option
If you need more time to prepare, you can opt out of the update before March 19, 2024.Note that if you opt out:- The existing functions will continue working on Node.js v16.- You won’t be able to create new functions after July 15, 2024.- You won’t be able to update existing functions after August 15, 2024.- You won’t receive future bug fixes, enhancements, and dependency updates to the functions runtime.Contact Segmentto opt-out or with any questions.
Node.js 18
Segment strongly recommends updating to Node.js v18 to benefit from future runtime updates, the latest security, and performance improvements.
Functions do not currently support importing dependencies, but you cancontact Segment Supportto request that one be added.
The following dependencies are installed in the function environment by default.
zlib v1.0.5exposed aszlib.zlib
uuidv5is exposed as an object. Useuuidv5.uuidv5to access its functions. For example:
zlib’s asynchronous methodsinflateanddeflatemust be used withasyncorawait. For example:
The following Node.js modules are available:
Other built-in Node.js modulesaren’t available.
For more information on using theaws-sdkmodule, see how toset up functions for calling AWS APIs.
Basic cache storage is available through thecacheobject, which has the following methods defined:
Some important notes about the cache:
Settings allow you to pass configurable variables to your function, which is the best way to pass sensitive information such as security tokens. For example, you might usesettingsas placeholders to use information such as an API endpoint and API key. This way, you can use the same code with different settings for different purposes. When you deploy a function in your workspace, you are prompted to fill out these settings to configure the function.
First, add a setting inSettingstab in the code editor:

ClickAdd Settingto add your new setting.

You can configure the details about this setting, which change how it’s displayed to anyone using your function:
As you change the values, a preview to the right updates to show how your setting will look and work.
ClickAdd Settingto save the new setting.
Once you save a setting, it appears in theSettingstab for the function. You can edit or delete settings from this tab.

Next, fill out this setting’s value in theTesttab, so you can run the function and verify that the correct setting value is passed. (This value is only for testing your function.)
Now that you’ve configured a setting and entered a test value, you can add code to read its value and run the function, as in the example below:
When you deploy your destination function in your workspace, you fill out the settings on the destination configuration page, similar to how you would configure a normal destination.
You must pass the settings object to the function at runtime. Functions can’t access the settings object when it’s stored as a global variable.
You can test your code directly from the editor in two ways:
ClickUse Sample Eventand select the source to use events from.

ClickRunto test your function with the event you selected.
You can also manually include your own JSON payload of a Segment event, instead of fetching a sample from one of your workspace sources.

If your function fails, you can check the error details and logs in theOutputsection.
Batch handlers are an extension of destination functions. When you define anonBatchhandler alongside the handler functions for single events (for example:onTrackoronIdentity), you’re telling Segment that the destination function can accept and handle batches of events.
Batching is available for destination and destination insert functions only.
Consider creating a batch handler if:
If a batched function receives too low a volume of events (under one event per second) to be worth batching, Segment may not invoke the batch handler.
Segment collects the events over a short period of time and combines them into a batch. The system flushes them when the batch reaches a certain number of events, or when the batch has been waiting for a specified wait time.
To create a batch handler, define anonBatchfunction within your destination function.
You can also use the “Default Batch” template found in the Functions editor to get started quickly.
TheonBatchhandler is an optional extension. Destination functions must still contain single event handlers as a fallback, in cases where Segment does not receive enough events to execute the batch.
The handler function receives an array of events. The events can be of any supported type and a single batch may contain more than one event type. Handler functions can also receive function settings. Here is an example of what a batch can look like:
For example, you could send the array of events to an external services batch endpoint:
Segment batches together any eventof any typethat it sees over a short period of time to increase batching efficiency and give you the flexibility to decide how batches are created. If you want to split batches by event type, you can implement this in your functions code by writing a handler.
If your downstream endpoint requires events of a single type, you can write a handler that groups events by type, and then handles the events.
By default, Functions waits up to 10 seconds to form a batch of 20 events. You can increase the number of events included in each batch (up to 400 events per batch) by contactingSegment support. Segment recommends users who wish to include fewer than 20 events per batch use destination functions without theonBatchhandler.
TheFunctions editing environmentsupports testing batch handlers.
To test the batch handler:
The Sample Event option tests single events only. You must use Manual Mode to add more than one event so you can test batch handlers.
The editor displays logs and request traces from the batch handler.
ThePublic APIFunctions/Preview endpoint also supports testing batch handlers. The payload must be a batch of events as a JSON array.
Standardfunction error typesapply to batch handlers. Segment attempts to retry the batch in the case of Timeout or Retry errors. For all other error types, Segment discards the batch. It’s also possible to report a partial failure by returning status of each event in the batch. Segment retries only the failed events in a batch until those events are successful or until they result in a permanent error.
For example, after receiving the responses above from theonBatchhandler, Segment only retriesevent_4andevent_5.
Once you finish building your destination function, clickConfigureto name it, then clickCreate Functionto save it.
Once you do that, the destination function appears on theFunctionspage in your workspace’s catalog.
If you’re editing an existing function, you canSavechanges without updating instances of the function that are already deployed and running.
You can also choose toSave & Deployto save the changes, and then choose which of the already-deployed functions to update with your changes.You might need additional permissionsto update existing functions.
If your function throws an error, execution halts immediately. Segment captures the event, any outgoing requests/responses, any logs the function might have printed, as well as the error itself.
Segment then displays the captured error information in theEvent Deliverypage for your destination. You can use this information to find and fix unexpected errors.
You can throwan error or a custom errorand you can also add helpful context in logs using theconsoleAPI. For example:
Warning:Do not log sensitive data, such as personally-identifying information (PII), authentication tokens, or other secrets. Avoid logging entire request/response payloads. TheFunction Logstab may be visible to other workspace members if they have the necessary permissions.
Functions execute only in response to incoming data, but the environments that functions run in are generally long-running. Because of this, you can use global variables to cache small amounts of information between invocations. For example, you can reduce the number of access tokens you generate by caching a token, and regenerating it only after it expires. Segment cannot make any guarantees about the longevity of environments, but by using this strategy, you can improve the performance and reliability of your Functions by reducing the need for redundant API requests.
This example code fetches an access token from an external API and refreshes it every hour:
Functions have specific roles which can be used foraccess managementin your Segment workspace.
Access to functions is controlled by two permissionsroles:
You also need additionalSource Adminpermissions to enable source functions, connect destination functions to a source, or to deploy changes to existing functions.
If you are aWorkspace OwnerorFunctions Admin, you can manage your function from theFunctionspage.
You can useDestination Event Deliveryto understand if Segment encounters any issues delivering your source data to destinations. Errors that the Function throws appear here.
If any of your deployed function instances are failing consistently, they will also appear inConnection Health.
In addition to usingDestination Filtersand thePrivacy Portalto manage which events and properties are sent to your destination function, you can reference the destination function directly in the integrations object of the Segment payload. For example:
In the example above, the integrations object directly references and enables the destination function (My Destination Function), located inside your workspace (My Workspace). Include the workspace name in parentheses, as shown in the example above. Like all items in the integration object, destination function and workspace names are case sensitive.
Yes, Functions access is logged in theAudit Trail, so user activity related to functions appears in the logs.
Yes, Segment retries invocations that throw RetryError or Timeout errors (temporary errors only). Segment’s internal system retries failed functions API calls for four hours with a randomized exponential backoff after each attempt. This substantially improves delivery rates.
Retrieswork the same for both functions and cloud-mode destinations in Segment.
No, Segment can’t guarantee the order in which the events are delivered to an endpoint.
No, destination functions are currently available as cloud-mode destinations only. Segment is in the early phases of exploration and discovery for supporting customer “web plugins” for custom device-mode destinations and other use cases, but this is unsupported today.
If you are a partner, looking to publish your destination and distribute your app through Segment catalog, visit theDeveloper Centerand check out the Segmentpartner docs.
TheEvent Delivery tabcontinues to show metrics for individual events, even if they are batched by your function code. For more information, seeDestination functions logs and errors.
A function’s use depends on the number of times it’s invoked, and the amount of time it takes to execute. When you enable batching, Segment invokes your functiononce per batchrather than once per event. The volume of events flowing through the function determines the number of batches, which determines the number of invocations.
If you’re sending your batch to an external service, the execution time of the function depends on the end-to-end latency of that service’s batch endpoint, which may be higher than an endpoint that receives a single event.
When data leaves Segment’s servers to go to various destinations (not including warehouses), Segment uses Amazon Web Services (AWS) and utilizes many different machines in order to send requests.
The IP addresses that are used to send these requests can be foundon Amazon’s website. If you want to allowlist these specific IP addresses, you need to allowlist all of the IP addresses from your workspace’s location range. Below are the ranges:
Yes, to do so, remove themessageIdand thewriteKeyfrom the payload in your Function code. Leaving either field on your payload will cause unexpected behavior that may cause your event to be delivered to the wrong source or to not be delivered at all.
Incorporating console.log() statements in your Destination Function code aids in debugging. However, logs generated by these statements will only be accessible in theEvent Deliveryview if the payloads encounter errors during processing. Logs from successfully processed payloads are not displayed.
The test function interface has a 4KB console logging limit. Outputs larger than this limit are not visible in the user interface.
This page was last modified: 19 Dec 2024
Questions? Problems? Need more info? Contact Segment Support for assistance!
Thanks for your feedback!
Can we improve this doc?Send us feedback!
On this page
Was this page helpful?
Thanks for your feedback!
Can we improvethis doc?Send us feedback!
Product
For Developers
Company
Support
© 2025 Segment.io, Inc.

Headings:
Destination Functions
Create a destination function
Code the destination function
Errors and error handling
Runtime and dependencies
Caching
Create settings and secrets
Test the destination function
Use sample events for testing
Test using manual input
Batching the destination function
When to use batching
Define the batch handler
Configure the event types within a batch
Configure your batch parameters
Test the batch handler
Handling batching errors
Save and deploy the function
Destination functions logs and errors
Caching in destination functions
Managing destination functions
Functions permissions
Editing and deleting functions
Monitoring destination functions
Data control
Destination functions FAQs
Can I see who made changes to a function?
Does Segment retry failed function invocations?
Are events guaranteed to send data in order?
Can I create a device-mode destination?
How do I publish a destination to the public Segment catalog?
How does batching affect visibility?
How does batching impact function use and cost?
Which IP addresses should be allowlisted?
Can I use a Destination Function to send data to another Segment source?
Can I view console.log() outputs in Destination Functions?
What is the maximum data size that can be displayed in console.logs() when testing a Function?
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/functions/insert-functions/
Paragraphs:
On this page
Use Destination Insert Functions to enrich, transform, or filter your data before it reaches downstream destinations.
Implement advanced data computation: Write custom computation, operations, and business logic on streaming data that you send to downstream destinations.
Enrich your data: Use destination insert functions with Segment’s Profile API or third party sources to add additional context to your data and create personalized customer experiences.
Support data compliance: Use destination insert functions to support data masking, encryption, decryption, improved PII data handling, and tokenization.
Customize filtration for your destinations: Create custom logic with nested if-else statements, regex, custom business rules, and more to filter event data.
Destination Insert Functions are not compatible with IP Allowlisting
For more information, see theIP Allowlistingdocumentation.
There are two ways you can access destination insert functions from your Segment space:
To create an insert function from Segment’s catalog:
For data to flow to your downstream destinations, you’ll need to connect your insert function to a destination:
Storage destinations are not compatible with Destination Insert Functions
You cannot connect an Insert Function to a storage destination at this time.
To access insert functions through the Destinations tab:
Use this page to edit and manage insert functions in your workspace.
You can also use this page toenable destination insert functionsin your workspace.
To prevent “Unsupported Event Type” errors, ensure your insert function handles all event types (page, track, identify, alias, group) that are expected to be sent to the destination. It is highly recommended totest the functionwith each event type to confirm they are being handled as expected.
Segment invokes a separate part of the function (called a “handler”) for each event type that you send to your destination insert function.
If you’ve configured adestination filterand the event doesn’t pass the filter, then your function isn’t invoked for that event as Segment applies destination filters before insert functions. The same is true for theintegrations object). If an event is configured with the integrations object not to go to a particular destination, then the insert function connected to that destination won’t be invoked.
The default source code template includes handlers for all event types. You don’t need to implement all of them - just use the ones you need, and skip the ones you don’t. For event types that you want to send through the destination, return the event in the respective event handlers.
Removing the handler for a specific event type results in blocking the events of that type from arriving at their destination. To keep an event type as is but still send it downstream, add areturn eventinside the event type handler statement.
Insert functions can define handlers for each message type in theSegment spec:
Each of the functions above accepts two arguments:
The example below shows a function that listens for “Track” events, and sends some details about them to an external service.
To change which event type the handler listens to, you can rename it to the name of the message type. For example, if you rename this functiononIdentify, it listens for “Identify” events instead.
To ensure the Destination processes an event payload modified by the function, return theeventobject at the handler’s end.
Functions’ runtime includes afetch()polyfill using anode-fetchpackage. Check out thenode-fetch documentationfor usage examples.
Segment considers a function’s execution successful if it finishes without error. You canthrowan error to create a failure on purpose. Use these errors to validate event data before processing it to ensure the function works as expected.
You canthrowthe following pre-defined error types to indicate that the function ran as expected, but the data was not deliverable:
The examples show basic uses of these error types.
If you don’t supply a function for an event type, Segment throws anEventNotSupportederror by default.
You can read more abouterror handlingbelow.
On March 26, 2024, Segment is upgrading the Functions runtime environment to Node.js v18, which is the current long-term support (LTS) release.
This upgrade keeps your runtime current with industry standards. Based on theAWS LambdaandNode.jssupport schedule, Node.js v16 is no longer inMaintenance LTS. Production applications should only use releases of Node.js that are inActive LTSorMaintenance LTS.
All new functions will use Node.js v18 starting March 26, 2024.
For existing functions, this change automatically occurs as you update and deploy an existing function. Segment recommends that you check your function post-deployment to ensure everything’s working. Your function may face issues due to the change in sytax between different Node.js versions and dependency compatibility.
Limited time opt-out option
If you need more time to prepare, you can opt out of the update before March 19, 2024.Note that if you opt out:- The existing functions will continue working on Node.js v16.- You won’t be able to create new functions after July 15, 2024.- You won’t be able to update existing functions after August 15, 2024.- You won’t receive future bug fixes, enhancements, and dependency updates to the functions runtime.Contact Segmentto opt-out or with any questions.
Node.js 18
Segment strongly recommends updating to Node.js v18 to benefit from future runtime updates, the latest security, and performance improvements.
Functions do not currently support importing dependencies, but you cancontact Segment Supportto request that one be added.
The following dependencies are installed in the function environment by default.
zlib v1.0.5exposed aszlib.zlib
uuidv5is exposed as an object. Useuuidv5.uuidv5to access its functions. For example:
zlib’s asynchronous methodsinflateanddeflatemust be used withasyncorawait. For example:
The following Node.js modules are available:
Other built-in Node.js modulesaren’t available.
For more information on using theaws-sdkmodule, see how toset up functions for calling AWS APIs.
Basic cache storage is available through thecacheobject, which has the following methods defined:
Some important notes about the cache:
A payload must come into the pipeline with the attributes that allow it to match your mapping triggers. You can’t use an Insert Function to change the event to match your mapping triggers. If an event comes into an Actions destination and already matches a mapping trigger, that mapping subscription will fire. If a payload doesn’t come to the Actions destination matching a mapping trigger, even if an Insert Function is meant to alter the event to allow it to match a trigger, it won’t fire that mapping subscription. Segment sees the mapping trigger first in the pipeline, so a payload won’t make it to the Insert Function at all if it doesn’t come into the pipeline matching a mapping trigger.
Unlike Source Functions and Destination Functions, which return multiple events, an Insert Function only returns one event. When the Insert Function receives an event, it sends the event to be handled by its configured mappings.
If you would likemultiple mappings triggered by the same event:
You can also configure the Insert Function to add additional data to the event’s payload before it’s handled by the mappings and configure the mapping’s available fields to reference the payload’s available fields.
You may want to consider thecontext object’savailable fields when adding new data to the event’s payload.
Settings allow you to pass configurable variables to your function, which is the best way to pass sensitive information such as security tokens. For example, you might usesettingsas placeholders to use information such as an API endpoint and API key. This way, you can use the same code with different settings for different purposes. When you deploy a function in your workspace, you are prompted to fill out these settings to configure the function.
First, add a setting inSettingstab in the code editor:

ClickAdd Settingto add your new setting.

You can configure the details about this setting, which change how it’s displayed to anyone using your function:
As you change the values, a preview to the right updates to show how your setting will look and work.
ClickAdd Settingto save the new setting.
Once you save a setting, it appears in theSettingstab for the function. You can edit or delete settings from this tab.

Next, fill out this setting’s value in theTesttab, so you can run the function and verify that the correct setting value is passed. (This value is only for testing your function.)
Now that you’ve configured a setting and entered a test value, you can add code to read its value and run the function, as in the example below:
When you deploy your destination insert function in your workspace, you fill out the settings on the destination configuration page, similar to how you would configure a normal destination.
You can manually test your code from the functions editor:
The Event Tester won’t make use of an Insert Function, show how an Insert Function impacts your data, or send data downstream through the Insert Function pipeline. The Event Tester is not impacted by an Insert Function at all. Use the Function tester rather than the Event Tester to see how your Insert Function impacts your data.
Once you finish building your insert function, clickNext: Configure & Createto name it, then clickCreate Functionto save it.
Once you do that, you’ll see the insert function from the Functions page in your catalog.
If you’re editing an existing function, you can save changes without updating the instances of the function that are already deployed and running.
You can also choose toSave & Deployto save the changes, then choose which already-deployed functions to update with your changes.
You may need additional permissions to update existing functions.
You need to enable your insert function for it to process your data.
To enable your insert function:
To prevent your insert function from processing data, toggle Enable Function off.
Batch handlers are an extension of insert functions. When you define anonBatchhandler alongside the handler functions for single events (for example,onTrackoronIdentity), you’re telling Segment that the insert function can accept and handle batches of events.
Batching is available for destination and destination insert functions only.
Consider creating a batch handler if:
You have a high-throughput function and want to reduce cost.When you define a batch handler, Segment invokes the function once perbatch, rather than once per event. As long as the function’s execution time isn’t adversely affected, the reduction in invocations should lead to a reduction in cost.
Your destination supports batching. When your downstream destination supports sending data downstream in batches you can define a batch handler to avoid throttling. Batching for functions is independent of batch size supported by the destination. Segment automatically handles batch formation for destinations.
If a batched function receives too low a volume of events (under one event per second) to be worth batching, Segment may not invoke the batch handler.
Segment collects the events over a short period of time and combines them into a batch. The system flushes them when the batch reaches a certain number of events, or when the batch has been waiting for a specified wait time.
To create a batch handler, define anonBatchfunction within your destination insert function. You can also use the “Default Batch” template found in the Functions editor to get started quickly.
TheonBatchhandler is an optional extension. Destination insert functions must still contain single event handlers as a fallback, in cases where Segment doesn’t receive enough events to execute the batch.
The handler function receives an array of events. The events can be of any supported type and a single batch may contain more than one event type. Handler functions can also receive function settings. Here is an example of what a batch can look like:
Segment batches together any eventof any typethat it sees over a short period of time to increase batching efficiency and give you the flexibility to decide how batches are created. If you want to split batches by event type, you can implement this in your functions code by writing a handler.
By default, Functions waits up to 10 seconds to form a batch of 20 events. You can increase the number of events included in each batch (up to 400 events per batch) by contactingSegment support. Segment recommends users who wish to include fewer than 20 events per batch use destination insert functions without theonBatchhandler.
TheFunctions editing environmentsupports testing batch handlers.
To test the batch handler:
The Sample Event option tests single events only. You must use Manual Mode to add more than one event so you can test batch handlers.
The editor displays logs and request traces from the batch handler.
ThePublic APIFunctions/Preview endpoint also supports testing batch handlers. The payload must be a batch of events as a JSON array.
Events in a batch can be filtered out using custom logic. The filtered events will be surfaced in theEvent Deliverypage with reason asFiltered at insert function
Standardfunction error typesapply to batch handlers. Segment attempts to retry the batch in the case of Timeout or Retry errors. For all other error types, Segment discards the batch.
Segment only attempts to send the event to your destination insert function again if aRetryerror occurs.
You can view Segment’s list ofIntegration Error Codesfor more information about what might cause an error.
If your function throws an error, execution halts immediately. Segment captures the event, any outgoing requests/responses, any logs the function might have printed, as well as the error itself.
Segment then displays the captured error information in theEvent Deliverypage for your destination. You can use this information to find and fix unexpected errors.
You can throwan error or a custom errorand you can also add helpful context in logs using theconsoleAPI. For example:
Don’t log sensitive data, such as personally-identifying information (PII), authentication tokens, or other secrets. Avoid logging entire request/response payloads. TheFunction Logstab may be visible to other workspace members if they have the necessary permissions.
Functions execute only in response to incoming data, but the environments that functions run in are generally long-running. Because of this, you can use global variables to cache small amounts of information between invocations. For example, you can reduce the number of access tokens you generate by caching a token, and regenerating it only after it expires. Segment cannot make any guarantees about the longevity of environments, but by using this strategy, you can improve the performance and reliability of your Functions by reducing the need for redundant API requests.
This example code fetches an access token from an external API and refreshes it every hour:
Functions have specific roles which can be used foraccess managementin your Segment workspace.
Access to functions is controlled by two permissionsroles:
You also need additionalSource Adminpermissions to enable source functions, connect destination functions to a source, or to deploy changes to existing functions.
If you are aWorkspace OwnerorFunctions Admin, you can manage your function from theFunctionspage.
Yes, Functions access is logged in theAudit Trail, so user activity related to functions appears in the logs.
Yes, Segment retries invocations that throw RetryError or Timeout errors (temporary errors only). Segment’s internal system retries failed functions API calls for four hours with a randomized exponential backoff after each attempt. This substantially improves delivery rates.
Retrieswork the same for both functions and cloud-mode destinations in Segment.
No, Segment can’t guarantee the order in which the events are delivered to an endpoint.
No, specifying an endpoint is not always required for insert functions. If your function is designed to transform or filter data internally—such as adding new properties to events or filtering based on existing properties—you won’t need to specify an external endpoint.
However, if your function aims to enrich event data by fetching additional information from an external service, then you must specify the endpoint. This would be the URL of the external service’s API where the enriched or captured data is sent.
No, Destination Insert Functions are currently available for use with Cloud Mode (server-side) destinations only. Segment is in the early phases of exploration and discovery for supporting customer web plugins for custom Device Mode destinations and other use cases, but this is unsupported today.
Insert Functions are only supported by Cloud Mode (server-side) destinations and aren’t compatible with Storage destinations.
Yes, you can connect an insert function to multiple destinations.
No, you can only connect one insert function to a destination.
Yes, you can have both destination filters and destination insert functions in the same connection.
Segment’s data pipeline applies Destination Filters before invoking Insert Functions.
There is an 120-Character limit for the insert function display name.
This error occurs because your insert function code might not be handling all event types (Page, Track, Identify, Alias, Group) that your destination supports. When these unlisted events pass through the function, they are rejected with the “Unsupported Event Type” error.
To resolve this, verify your insert function includes handlers for all expected event types and returns the event object for each. Here’s an example of how you can structure your insert function to handle all event types:
By including handlers for all the major event types, you ensure that all supported events are processed correctly, preventing the “Unsupported Event Type” error. Always test your updated code before implementing it in production.
The test function interface has a 4KB console logging limit. Outputs surpassing this limit won’t be visible in the user interface.

Headings:
Destination Insert Functions
Create destination insert functions
Using the catalog
Using the Destinations tab
Code the destination insert function
Errors and error handling
Runtime and dependencies
Caching
Insert Functions and Actions destinations
Create settings and secrets
Test the destination insert function
Save and deploy the destination insert function
Enable the destination insert function
Batching the destination insert function
When to use batching
Define the batch handler
Configure the event types within a batch
Configure your batch parameters
Test the batch handler
Handling filtering in a batch
Handling batching errors
Destination insert functions error types
Destination insert functions logs
Caching in destination insert functions
Managing destination insert functions
Functions permissions
Editing and deleting functions
Destination insert functions FAQs
Can I see who made changes to a function?
Does Segment retry failed function invocations?
Are events guaranteed to send data in order?
Do I Need to specify an endpoint for my Insert function?
Can I use Insert Functions with Device Mode destinations?
Can I use Insert Functions with Storage destinations?
Can I connect an insert function to multiple destinations?
Can I connect multiple insert functions to one destination?
Can I have destination filters and a destination insert function in the same connection?
Are insert functions invoked before or after Destination Filters are applied?
Why am I receiving a 500 Internal Error when saving the same of the destination insert function?
Why does the Event Delivery tab show “Unsupported Event Type” errors for events supported by the destination after I enabled an insert function?
What is the maximum data size that can be displayed in console.logs() when testing a Function?
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/functions/copilot/
Paragraphs:
On this page
Functions Copilot helps you generate JavaScript code for functions using natural language prompts. For more information about the language model used to generate JavaScript code, see theFunctions Copilot Nutrition Facts Label.
Functions Copilot improves efficiency and productivity by streamlining the process of creating and managing custom functions.
Functions Copilot can help you:
This table lists example prompts you can use with Functions Copilot:
Follow this guidance when you use Functions Copilot:
Keep the following limitations in mind as you work with Functions Copilot:

Headings:
Functions Copilot
Functions Copilot benefits
Example prompts
Best practices and limitations
Limitations
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/functions/functions-copilot-nutrition-facts/
Paragraphs:
Twilio’sAI Nutrition Factsprovide an overview of the AI feature you’re using, so you can better understand how the AI is working with your data. Function Copilot’s AI qualities are outlined in the following Nutrition Facts label. For more information, including the glossary regarding the AI Nutrition Facts label, refer to theAI Nutrition Factspage.


AI Nutrition FactsCustomer AI Functions Copilot
DescriptionFunctions Copilot is an AI-powered coding assistant designed to streamline the development of custom integrations, and enrich and transform Segment Functions.
Privacy Ladder Level1
Feature is OptionalYes
Model TypeGenerative
Base ModelOpenAI - GPT-4
Trust Ingredients
Base Model Trained with Customer DataNo
Customer Data Shared with Model VendorNo
Training Data AnonymizedN/A
Data DeletionYes
Human in the LoopYes
Data RetentionN/A
Input/Output ConsistencyYes
Other ResourcesLearn more at:https://twilio.com/en-us/customer-ai

Headings:
Functions Copilot Nutrition Facts Label
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/functions/environment/
Paragraphs:
On this page
Segment Functions create reusable code that can be run in your Segment workspace either as sources to format incoming events, or as destinations, to handle specific event types.
When you create a function, write code for it, and save it, the function appears in the Catalog in your workspaceonly. You can then deploy that function in your workspace just as you would a conventional source or destination.
Access to Functions is controlled by specificaccess management roles. You may need additional access to create and deploy functions.
OnlyFunctions adminscan create or edit functions.
Select the type of function you want to build, and clickBuild.
When you clickBuild, a code editor appears. Different template code is available depending on which type of function you created.
After you clickCreate Function, the function appears on theFunctions catalog pagein your workspace.
If you are aWorkspace OwnerorFunctions Admin, you can manage your function from theFunctions catalog page.
If you’re editing an existing function, you canSavechanges without changing the behavior of existing instances of the function.
You can also choose toSave & Deployto push changes to all, or specific functions in your workspace that are already deployed. You might need additional permissions to deploy these changes.
You have the option to test your functions code with either a sample event or by loading a default event that you can customize yourself.
Once the payload you want to test is ready, clickRun.
If you create settings in your function, then you need to fill in the setting values before clickingRun.
You must be aWorkspace OwnerorSource Adminto connect an instance of your function in your workspace.
If you’re editing an existing function, you canSavechanges without changing the behavior of your deployed function. You can also choose toSave & Deployto push changes to all, or specific functions in your workspace that are already deployed.
When you deploy your destination function in your workspace, you fill out the settings on the destination configuration page, similar to how you would configure a normal destination.
With Functions Versioning, you can access a complete change history for each source or destination function. View version history and creation details, then use a unified or split display to compare code and restore previous versions of a function.
To view the version history of a function:
Select previous versions to compare code using aunifiedorsplitview. With the split view, Segment displays the latest version on the left and the version you’ve selected on the right.
Unified and split compare screens are read-only. While you can copy code, you can’t make changes directly from these screens.
In the Version History panel, Segment displaysLATESTandDEPLOYEDlabels that represent a function version state. You’ll see theLATESTversion at the top.
Segment labels a version as theLATESTwhen:
TheDEPLOYEDversion is the function version that’s currently deployed.
To restore a previous function version:
You can use Functions Versioning with Segment’sPublic APIto retrieve version history records and source code, as well as to restore previous versions.
Here are some Public API use case examples:
Get Version history: Use the/versionsendpoint to retrieve a list of version records and metadata of a certain page size. You can also use this endpoint to get version source code for a given version ID.
Restore a previous version: Use the/restoreendpoint to restore a previous function version. This creates a new version with the same source as the version you are restoring.
Create or update versions: Create or update a function to add a version record and save the source code.
Deploy a function: Use the Public API to deploy a function. After you deploy, Segment marks the function version asDEPLOYED. Learn more about function version states in theLatest and deployed versionssection.
View Segment’sPublic APIdocs for more information on how to use Functions Versioning with the Public API.
Functions have specific roles which can be used foraccess managementin your Segment workspace.
Access to functions is controlled by two permissionsroles:
You also need additionalSource Adminpermissions to enable source functions, connect destination functions to a source, or to deploy changes to existing functions.
Settings allow you to pass configurable variables to your function, which is the best way to pass sensitive information such as security tokens. For example, you might usesettingsas placeholders to use information such as an API endpoint and API key. This way, you can use the same code with different settings for different purposes. When you deploy a function in your workspace, you are prompted to fill out these settings to configure the function.
First, add a setting inSettingstab in the code editor:

ClickAdd Settingto add your new setting.

You can configure the details about this setting, which change how it’s displayed to anyone using your function:
As you change the values, a preview to the right updates to show how your setting will look and work.
ClickAdd Settingto save the new setting.
Once you save a setting, it appears in theSettingstab for the function. You can edit or delete settings from this tab.

On March 26, 2024, Segment is upgrading the Functions runtime environment to Node.js v18, which is the current long-term support (LTS) release.
This upgrade keeps your runtime current with industry standards. Based on theAWS LambdaandNode.jssupport schedule, Node.js v16 is no longer inMaintenance LTS. Production applications should only use releases of Node.js that are inActive LTSorMaintenance LTS.
All new functions will use Node.js v18 starting March 26, 2024.
For existing functions, this change automatically occurs as you update and deploy an existing function. Segment recommends that you check your function post-deployment to ensure everything’s working. Your function may face issues due to the change in sytax between different Node.js versions and dependency compatibility.
Limited time opt-out option
If you need more time to prepare, you can opt out of the update before March 19, 2024.Note that if you opt out:- The existing functions will continue working on Node.js v16.- You won’t be able to create new functions after July 15, 2024.- You won’t be able to update existing functions after August 15, 2024.- You won’t receive future bug fixes, enhancements, and dependency updates to the functions runtime.Contact Segmentto opt-out or with any questions.
Node.js 18
Segment strongly recommends updating to Node.js v18 to benefit from future runtime updates, the latest security, and performance improvements.
Functions do not currently support importing dependencies, but you cancontact Segment Supportto request that one be added.
The following dependencies are installed in the function environment by default.
zlib v1.0.5exposed aszlib.zlib
uuidv5is exposed as an object. Useuuidv5.uuidv5to access its functions. For example:
zlib’s asynchronous methodsinflateanddeflatemust be used withasyncorawait. For example:
The following Node.js modules are available:
Other built-in Node.js modulesaren’t available.
For more information on using theaws-sdkmodule, see how toset up functions for calling AWS APIs.
Basic cache storage is available through thecacheobject, which has the following methods defined:
Some important notes about the cache:

Headings:
The Functions Editing Environment
Creating functions
Editing a function
Testing a function
Deploying source functions
Deploying destination functions
Functions Versioning
View and compare version history
LATESTandDEPLOYEDversions
Restore a previous version
Use Versioning with Segment’s Public API
Functions permissions
️Settings and secrets
Runtime and dependencies
Caching
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/functions/usage/
Paragraphs:
On this page
Functions are billed to your account using the total execution time per month.
Anindividual function’s execution timeis the total time it takes for the function to process events, including mapping, transformations, and requests to external APIs. Generally, requests to external APIs can greatly add to your total execution time.
Yourtotal execution timeis the execution time for all of your active functions accumulated over the course of a month. You can see your current execution time on theFunctions tab of the Usage pagein each workspace. You will receive notifications of your usage when you’ve reached 75%, 90%, and 100% of your allotted execution time.
The amount of time you are allotted changes depending on yourSegment pricing plan.
Segment measures execution time from when the function first receives an event to the time the function either returns successfully or throws an error. If Segment retries your function (for example, if there was a timeout), those retries also count as billable execution time.
Starting on April 8, 2021 Functions usage is measured in millisecond increments. This makes your usage and billing much more precise. Prior to this change, Functions was measured in 100ms increments, and then rounded up. For example, a function that took 80ms to complete was previously billed as 100ms. Using the new usage calculation, it is billed as 80ms.
Functions have a timeout of five seconds. If a function takes longer than five seconds, execution halts and the function is retried periodically for up to four hours.
Execution time can vary widely between use cases, so it is extremely difficult to predict. The best way is to look at the function’s actual execution time and multiply it by the event volume.
Another way to provide a rough estimate is to use an expected source function time of 100ms per invocation, and expected destination function time at 200ms per invocation:
Note:Test runs are generally slower than the time it takes a function to run once it’s deployed. For more accurate estimates, base your estimates on sending data into a production function, and not on timing the test runs.
You can (and should!) useDestination Filtersto reduce the volume of events reaching your function. Filtering events with a Destination Filter prevents the Function from being invoked for that event entirely.
In the most cases, functions are slow due to external requests using thefetch()call. The external API may be under heavy load or it may simply take a long time to process your request.
If you’re making many requests that could be done in parallel, ensure that you’re not doing them sequentially. If the external API takes 400ms to respond and you issue 10 requests, it would take four seconds to do them sequentially versus 400ms to do them in parallel. For example, if you’re waiting for requests to complete inside of a loop you’re making your requests sequentially:
Instead, consider making an array of async requests that are running in parallel and then usingPromise.all()to wait for all of them to complete:
If you’re only issuing a single request in your function and it is slow, you might want to contact the owner of the external API for support.
Each workspace has a default limit of 25 Functions in total across Source, Insert, and Destination Functions. If you want to create more, please reach out toSegment.

Headings:
Functions usage limits
Measuring execution time
Execution timeouts
Estimating execution time
Improving speed of external requests
Default limit number
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/functions/aws-apis/
Paragraphs:
Theaws-sdkmodule is built-in, which allows you to make calls to AWS services in your own AWS accounts. The AWS SDK requires additional setup to ensure access to your AWS resources is secure. This page describes the process for allowing your functions to securely call AWS APIs in your AWS account.
To set up your functions to call AWS APIs:
Below is an example destination function that uploads each event received to an S3 bucket (configured using additional “S3 Bucket” and “S3 Bucket Region” settings). It uses the built-in local cache to retain S3 clients between requests to minimize processing time and to allow different instances of the function to use different IAM roles.

Headings:
Set up functions for calling AWS APIs
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/
Paragraphs:
Off-the-shelf analytics tools (like Google Analytics and Mixpanel) offer quick and easy insights about common business questions, and often meet the needs of marketing teams and product managers. However, data analysts and data scientists need access to an organization’s raw data to derive deeper and more customized insights to support their organization.
Only users with Business or Team plans can add Warehouse destinations.
Segment offers severalData Storage Destinationsto help you store your raw Segment data, including:
Although the sharing economy is eroding the idea of “ownership,” when it comes to analytics data, we strongly believe that you should own it.

Headings:
Data Storage overview
Analytics Academy: Why you should own your data
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/catalog/
Paragraphs:
Beta
Was this page helpful?
Thanks for your feedback!
Can we improvethis doc?Send us feedback!
Product
For Developers
Company
Support
© 2025 Segment.io, Inc.

Headings:
Data Storage catalog
AWS S3
Amazon S3
Azure Synapse Analytics Warehouse
BigQuery
Databricks
Google Cloud Storage
IBM Db2 Warehouse
Postgres
Redshift
Segment Data Lakes
Snowflake
Crawling: https://segment.com/docs/connections/storage/data-lakes/
Paragraphs:
Data Lakes is available for the listed account plans only.See theavailable plans, orcontact Support.
On this page
Segment Data Lakes will enter Limited Access in October 2024
After Segment Data Lakes enters Limited Access, new customers will no longer be able to create Segment Data Lake instances. Existing Segment customers with Data Lakes instances will continue to receive data and can create Data Lakes Destinations.
Segment recommends considering alternative solutions, likeAWS S3orDatabricks.
Adata lakeis a centralized cloud storage location that holds structured and unstructured data.
Data lakes typically have four layers:

Segment Data Lakes sends Segment data to a cloud data store, either AWS S3 or Azure Data Lake Storage Gen2 (ADLS), in a format optimized to reduce processing for data analytics and data science workloads. Segment data is great for building machine learning models for personalization and recommendations, and for other large scale advanced analytics. Data Lakes reduces the amount of processing required to get real value out of your data.
Segment Data Lakes deletion policies
Segment Data Lakes (AWS) and Segment Data Lakes (Azure) do not support Segment’suser deletion and suppressioncapabilities, as you retain your data in systems that you manage.
To learn more about Segment Data Lakes, check out the Segment blog postIntroducing Segment Data Lakes.
Segment supports Data Lakes hosted on two cloud providers: Amazon Web Services (AWS) and Microsoft Azure. Each cloud provider has a similar system for managing data, but offer different query engines, post-processing systems, and analytics options.
Data Lakes store Segment data in S3 in a read-optimized encoding format (Parquet) which makes the data more accessible and actionable. To help you zero-in on the right data, Data Lakes also creates logical data partitions and event tables, and integrates metadata with existing schema management tools, such as the AWS Glue Data Catalog. The resulting data set is optimized for use with systems like Spark, Athena, EMR, or machine learning vendors like DataBricks or DataRobot.

Segment sends data to S3 by orchestrating the processing in an EMR (Elastic MapReduce) cluster within your AWS account using an assumed role. Customers using Data Lakes own and pay AWS directly for these AWS services.

Data Lakes store Segment data in ADLS in a read-optimized encoding format (Parquet) which makes the data more accessible and actionable. To help you zero-in on the right data, Data Lakes also creates logical data partitions and event tables, and integrates metadata with existing schema management tools, like the Hive Metastore. The resulting data set is optimized for use with systems like Power BI and Azure HDInsight or machine learning vendors like Azure Databricks or Azure Synapse Analytics.

For detailed Segment Data Lakes (Azure) setup instructions, see theData Lakes setup page.
When setting up your data lake using theData Lakes catalog page, be sure to consider the EMR and AWS IAM components listed below.
Data Lakes uses an EMR cluster to run jobs that load events from all sources into Data Lakes. TheAWS resources portion of the set up instructionssets up an EMR cluster using them5.xlargenode type. Data Lakes keeps the cluster always running, however the cluster auto-scales to ensure it’s not always running at full capacity. Check the Terraform module documentation for theEMR specifications.
Data Lakes uses an IAM role to grant Segment secure access to your AWS account. The required inputs are:
To connect Segment Data Lakes (Azure), you must set up the following components in your Azure environment:
For more information about configuring Segment Data Lakes (Azure), see theData Lakes setup page.
Segment Data Lakes applies a standard schema to make the raw data easier and faster to query. Partitions are applied to the S3 data for granular access to subsets of the data, schema components such as data types are inferred, and a map of the underlying data structure is stored in a Glue Database.
Segment partitions the data in S3 by the Segment source, event type, then the day and hour an event was received by Segment, to ensure that the data is actionable and accessible.
The file path looks like:s3://<top-level-Segment-bucket>/data/<source-id>/segment_type=<event type>/day=<YYYY-MM-DD>/hr=<HH>
Here are a few examples of what events look like:s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=identify/day=2020-05-11/hr=11/s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=identify/day=2020-05-11/hr=12/s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=identify/day=2020-05-11/hr=13/
s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=page_viewed/day=2020-05-11/hr=11/s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=page_viewed/day=2020-05-11/hr=12/s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=page_viewed/day=2020-05-11/hr=13/
By default, the date partition structure isday=<YYYY-MM-DD>/hr=<HH>to give you granular access to the S3 data. You can change the partition structure during theset up process, where you can choose from the following options:
Data Lakes stores the inferred schema and associated metadata of the S3 data in AWS Glue Data Catalog. This metadata includes the location of the S3 file, data converted into Parquet format, column names inferred from the Segment event, nested properties and traits which are now flattened, and the inferred data type.

New columns are appended to the end of the table in the Glue Data Catalog as they are detected.
The schema inferred by Segment is stored in a Glue database within Glue Data Catalog. Segment stores the schema for each source in its own Glue database to organize the data so it is easier to query. To make it easier to find, Segment writes the schema to a Glue database named using the source slug by default. The database name can be modified from the Data Lakes settings.
The recommended IAM role permissions grant Segment access to create the Glue databases on your behalf. If you do not grant Segment these permissions, you must manually create the Glue databases for Segment to write to.
Segment Data Lakes (Azure) applies a consistent schema to make raw data accessible for queries. A transformer automatically calculates the desired schema and uploads a schema JSON file for each event type to your Azure Data Lake Storage (ADLS) in the/staging/directory.
Segment partitions the data in ALDS by the Segment source, event type, then the day and hour an event was received by Segment, to ensure that the data is actionable and accessible.
The file path looks like this:<storage-account-name>/<container-name>/staging/<source-id>/
Data Lakes infers the data type for an event it receives. Groups of events are polled every hour to infer the data type for that each event.
The data types supported in Segment Data Lakes are:
Once Data Lakes sets a data type for a column, all subsequent data will attempt to be cast into that data type. If incoming data does not match the data type, Data Lakes tries to cast the column to the target data type.
Size mismatch
If the data type in Glue is wider than the data type for a column in an on-going sync (for example, a decimal vs integer, or string vs integer), then the column is cast to the wider type in the Glue table. If the column is narrower (for example, integer in the table versus decimal in the data), the data might be dropped if it cannot be cast at all, or in the case of numbers, some data might lose precision. The original data in Segment remains in its original format, so you can fix the types andreplayto ensure no data is lost. Learn more about type casting by reading theW3School’s Java Type Castingpage.
Data mismatch
If Data Lakes sees a bad data type, for example text in place of a number or an incorrectly formatted date, it attempts a best effort conversion to cast the field to the target data type. Fields that cannot be cast may be dropped. You can also correct the data type in the schema to the desired type and Replay to ensure no data is lost.Contact Segment Supportif you find a data type needs to be corrected.
In addition to Segment’s99% guarantee of no duplicatesfor data within a 24 hour look-back window, Data Lakes have another layer of deduplication to ensure clean data in your Data Lake. Segment removes duplicate events at the time your Data Lake ingests data.  Data Lakes deduplicate any data synced within the last seven days, based on themessageIdfield.
The Data Lakes and Warehouses products are compatible using a mapping, but do not maintain exact parity with each other. This mapping helps you to identify and manage the differences between the two storage solutions, so you can easily understand how the data in each is related. You canread more about the differences between Data Lakes and Warehouses.
When you use Data Lakes, you can either use Data Lakes as youronlysource of data and query all of your data directly from S3 or ADLS or you can use Data Lakes in addition to a data warehouse.
Data Lakes supports data from all event sources, including website libraries, mobile, server and event cloud sources. Data Lakes doesn’t support loadingobject cloud source data, as well as the users and accounts tables from event cloud sources.
Segment doesn’t support User deletions in Data Lakes, but supportsuser suppression.
As the data schema evolves, both Segment Data Lakes (AWS) and Segment Data Lakes (Azure) can detect new columns and add them to Glue Data Catalog or Azure Data Lake Storage (ADLS). However, Segment can’t update existing data types. To update Segment-created data types, please reach out toAWS SupportorAzure Support.
Data Lakes has no direct integration withProtocols.
Any changes to events at the source level made with Protocols also change the data for all downstream destinations, including Data Lakes.
Mutated events- If Protocols mutates an event due to a rule set in the Tracking Plan, then that mutation appears in Segment’s internal archives and reflects in your data lake. For example, if you use Protocols to mutate the eventproduct_idto beproductID, then the event appears in both Data Lakes and Warehouses asproductID.
Blocked events- If a Protocols Tracking Plan blocks an event, the event isn’t forwarded to any downstream Segment destinations, including Data Lakes. However events which are only marked with a violationarepassed to Data Lakes.
Data types and labels available in Protocols aren’t supported by Data Lakes.
Data Lakes offers 12 syncs in a 24 hour period and doesn’t offer a custom sync schedule or selective sync.
You can find details on Amazon’spricing for Gluepage. For reference, Data Lakes creates 1 table per event type in your source, and adds 1 partition per hour to the event table.
You can find details on Microsoft’spricing for Azurepage. For reference, Data Lakes creates 1 table per event type in your source, and adds 1 partition per hour to the event table.
AWS Glue has limits across various factors, such as number of databases per account, tables per account, and so on. See thefull list of Glue limitsfor more information.
The most common limits to keep in mind are:
Segment stops creating new tables for the events after you exceed this limit. However you can contact your AWS account representative to increase these limits.
You should also read theadditional considerations in Amazon’s documentationwhen using AWS Glue Data Catalog.
Segment Data Lakes (Azure) supports the following analytics tools:

Headings:
Segment Data Lakes Overview
How Data Lakes work
How Segment Data Lakes (AWS) works
How Segment Data Lakes (Azure) works
Set up Segment Data Lakes (Azure)
Set up Segment Data Lakes (AWS)
EMR
AWS IAM role
Set up Segment Data Lakes (Azure)
Data Lakes schema
Segment Data Lakes (AWS) schema
S3 partition structure
AWS Glue data catalog
Glue database
Segment Data Lakes (Azure) schema
Data types
Schema evolution
Data Lake deduplication
Using a Data Lake with a Data Warehouse
FAQ
Can I send all of my Segment data into Data Lakes?
Are user deletions and suppression supported?
How does Data Lakes handle schema evolution?
How does Data Lakes work with Protocols?
How frequently does my Data Lake sync?
What is the cost to use AWS Glue?
What is the cost to use Microsoft Azure?
What limits does AWS Glue have?
What analytics tools are available to use with Segment Data Lakes (Azure)?
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/catalog/data-lakes/
Paragraphs:
On this page
Segment Data Lakes provide a way to collect large quantities of data in a format that’s optimized for targeted data science and data analytics workflows. You can readmore information about Data Lakesand learnhow they differ from Warehousesin Segment’s Data Lakes documentation.
Segment supports two type of data-lakes:
Lake Formation
You can also set up your Segment Data Lakes usingLake Formation, a fully managed service built on top of the AWS Glue Data Catalog.
To set up Segment Data Lakes, create your AWS resources, enable the Segment Data Lakes destination in the Segment app, and verify that your Segment data is synced to S3 and Glue.
Before you set up Segment Data Lakes, you need the following resources:
You can use theopen source Terraform moduleto automate much of the set up work to get Data Lakes up and running. If you’re familiar with Terraform, you can modify the module to meet your organization’s needs, however Segment guarantees support only for the template as provided. The Data Lakes set up uses Terraform v0.12+. To support more versions of Terraform, the AWS provider must use v4, which is included in the examplemain.tf.
You can also use Segment’smanual setup instructionsto configure these AWS resources, if you prefer.
The Terraform module and manual setup instructions both provide a base level of permissions to Segment (for example, the correct IAM role to allow Segment to create Glue databases on your behalf). If you want stricter permissions, or other custom configurations, you can customize these manually.
After you set up the necessary AWS resources, the next step is to set up the Data Lakes destination within Segment:
In theSegment App, clickAdd Destination, then search for and selectData Lakes.
ClickConfigure Data Lakesand select the source to connect to the Data Lakes destination.Warning: You must add the Workspace ID to the external ID list in the IAM policy, or else the source data cannot be synced to S3.
You must individually connect each source to the Data Lakes destination. However, you can copy the settings from another source by clicking…(“more”) (next to the button for “Set up Guide”).
(Optional)Glue Database Name: Optional advanced setting to change the name of the Glue Database which is set to the source slug by default. Each source connected to Data Lakes must have a different Glue Database name otherwise data from different sources will collide in the same database.
Once the Data Lakes destination is enabled, the first sync will begin approximately 2 hours later.
You will see event data andsync reportspopulated in S3 and Glue after the first sync successfully completes. However if aninsufficient permissionorinvalid settingis provided during set up, the first data lake sync will fail.
To receive sync failure alerts by email, subscribe to theStorage Destination Sync Failedactivity email notification within theApp Settings > User Preferences >Notification Settings.
Sync Failedemails are sent on the 1st, 5th, and 20th sync failure. Learn more about the types of errors which can cause sync failures in Segment’sSync errorsdocs.
If you want to add historical data to your data set using areplay of historical datainto Data Lakes,contact the Segment Support teamto request one.
Replay processing time can vary depending on the volume of data and number of events in each source. If you decide to run a Replay, Segment recommends that you start with data from the last six months to get started, and then replay additional data if you find you need more.
Segment creates a separate EMR cluster to run replays, then destroys it when the replay finishes. This ensures that regular Data Lakes syncs are not interrupted, and helps the replay finish faster.
To set up Segment Data Lakes (Azure), create your Azure resources and then enable the Data Lakes destination in the Segment app.
Before you can configure your Azure resources, you must complete the following prerequisites:
Before continuing, note the Location, Storage account name, and the Azure storage container name: you’ll need this information when configuring the Segment Data Lakes (Azure) destination in the Segment app.
Before continuing, note the MySQL server URL, username and password for the admin account, and your database name: you’ll need this information when configuring the Segment Data Lakes (Azure) destination in the Segment app.
Databricks pricing tier
If you create a Databricks instance only for Segment Data Lakes (Azure) usage, only the standard pricing tier is required. However, if you use your Databricks instance for other applications, you may require premium pricing.
Before continuing, note the Cluster ID, Workspace name, Workspace URL, and the Azure Resource Group for your Databricks Workspace: you’ll need this information when configuring the Segment Data Lakes (Azure) destination in the Segment app.
Before continuing, note the Client ID and Client Secret for your Service Principal: you’ll need this information when configuring the Segment Data Lakes (Azure) destination in the Segment app.
Optional configuration settings for log4j vulnerability
While Databricks released a statement that clusters are likely unaffected by the log4j vulnerability, out of an abundance of caution, Databricks recommends updating to log4j 2.15+ or adding the following options to the Spark configuration:spark.driver.extraJavaOptions "-Dlog4j2.formatMsgNoLookups=true"spark.executor.extraJavaOptions "-Dlog4j2.formatMsgNoLookups=true"
After you set up the necessary resources in Azure, the next step is to set up the Data Lakes destination in Segment:
Instead of manually configuring your Data Lake, you can create it using the script in theterraform-segment-data-lakesGitHub repository.
This script requires Terraform versions 0.12+.
Before you can run the Terraform script, create a Databricks workspace in the Azure UI using the instructions inStep 4 - Set up Databricks. Note theWorkspace URL, as you will need it to run the script.
In the setup file, set the following local variables:
After you’ve configured your local variables, run the following commands:
Running theplancommand gives you an output that creates 19 new objects, unless you are reusing objects in other Azure applications. Running theapplycommand creates the resources and produces a service principal password you can use to set up the destination.
No, Data Lakes automatically creates one Glue database per source. This database uses the source slug as its name.
Four roles are created when you set up Data Lakes using Terraform. You add thearn:aws:iam::$ACCOUNT_ID:role/segment-data-lake-iam-rolerole to the Data Lakes Settings page in the Segment web app.
The roles which Data Lakes assigns during set up are:
segment_emr_service_role- Restricted role that can only be assumed by the EMR service. This is set up based onAWS best practices.
The module doesn’t create a new S3 bucket so you can re-use an existing bucket for your Data Lakes.
Yes, the S3 bucket and the EMR cluster must be in the same region.
To connect a new source to Data Lakes:
Yes, you can configure multiple sources to use the same EMR cluster. Segment recommends that the EMR cluster only be used for Data Lakes to ensure there aren’t interruptions from non-Data Lakes job.
If you don’t see data after enabling a source, check the following:
If all of these look correct and you’re still not seeing any data, pleasecontact the Support team.
Theoutputtables are temporary tables Segment creates when loading data. They are deleted after each sync.
Yes, you can create new directories in S3 without interfering with Segment data.
Do not modify, or create additional directories with the following names:
Partitionedjust means that the table has partition columns (day and hour). All tables are partitioned, so you should see this on all table names.
You can use the following command to create external tables in Spectrum to access tables in Glue and join the data with Redshift:
Run theCREATE EXTERNAL SCHEMAcommand:
Replace:
Yes, your storage account and Databricks instance should be in the same region.
Segment Data Lakes (Azure) supports the following post-processing tools:
If you encounter errors related to your Databricks database, try adding the following line to the config:
After you’ve added to your config, restart your cluster so that your changes can take effect. If you continue to encounter errors,contact Segment Support.
Check your Spark configs to ensure that the information you entered about the database is correct, then restart the cluster. The Databricks cluster automatically initializes the Hive Metastore, so an issue with your config file will stop the table from being created.  If you continue to encounter errors,contact Segment Support.

Headings:
Set Up Segment Data Lakes
Set up Segment Data Lakes (AWS)
Prerequisites
Step 1 - Set up AWS resources
Step 2 - Enable Data Lakes destination
Step 3 - Verify data is synced to S3 and Glue
(Optional) Step 4 - Replay historical data
Set up Segment Data Lakes (Azure)
Prerequisites
Step 1 - Create an ALDS-enabled storage account
Step 2 - Set up Key Vault
Step 3 - Set up Azure MySQL database
Step 4 - Set up Databricks
Step 5 - Set up a Service Principal
Step 6 - Configure Databricks Cluster
Step 7 - Enable the Data Lakes destination in the Segment app
(Optional) Set up your Segment Data Lake (Azure) using Terraform
FAQ
Segment Data Lakes
Do I need to create Glue databases?
What IAM role do I use in the Settings page?
What level of access do the AWS roles have?
Why doesn’t the Data Lakes Terraform module create an S3 bucket?
Does my S3 bucket need to be in the same region as the other infrastructure?
How do I connect a new source to Data Lakes?
Can I configure multiple sources to use the same EMR cluster?
Why don’t I see any data in S3 or Glue after enabling a source?
What are “Segment Output” tables in S3?
Can I make additional directories in the S3 bucket Data Lakes is using?
What does “partitioned” mean in the table name?
How can I use AWS Spectrum to access Data Lakes tables in Glue, and join it with Redshift data?
Segment Data Lakes (Azure)
Does my ALDS-enabled storage account need to be in the same region as the other infrastructure?
What analytics tools are available to use with my Segment Data Lake (Azure)?
What can I do to troubleshoot my Databricks database?
What do I do if I get a “Version table does not exist” error when setting up the Azure MySQL database?
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/data-lakes/sync-reports/
Paragraphs:
Data Lakes is available for the listed account plans only.See theavailable plans, orcontact Support.
On this page
Segment Data Lakes generates reports with operational metrics about each sync to your data lake so you can monitor sync performance. These sync reports are stored in your S3 bucket and Glue Data Catalog. This means you have access to the raw data, so you can query it to answer questions and set up alerting and monitoring tools.
Your sync_report table stores all of your sync data. You can query it to answer common questions about data synced to your data lake.
The table has the following columns in its schema:
The Glue Database named__segment_datalakestores the schema of thesync_reportstable. The__segment_datalakedatabase has the following format:
Thesync_reportstable is available in S3 and Glue only once a sync completes. Sync reports are not available for syncs in progress.
Data Lakes sync reports are stored in Glue and in S3.
Segment automatically creates a Glue Database and table when you set up Data Lakes to store all sync report tables. The Glue Database is named__segment_datalake, and the table is namedsync_reports.
The S3 structure is:s3://my-bucket/segment-data/reports/day=YYYY-MM-DD/source=$SOURCE_ID/run_id=$RUN_ID/report.json
The data in the sync reports is stored in JSON format to ensure that it is human-readable and can be processed by other systems.
Each table involved in the sync is a separate JSON object that contains the sync metrics for the data loaded to that table.
The example below shows the raw JSON object for asuccessfulsync report.
The example below shows the raw JSON object for afailedsync report.
You can use SQL to query your Sync Reports table to explore and analyze operational sync metrics.
A few helpful and commonly used queries are included below.
The following error types can cause your data lake syncs to fail:
If Data Lakes does not have the correct access permissions for S3, Glue, and EMR, your syncs will fail.
If permissions are the problem, you might see one of the following permissions-related error messages:
Check the set up guideto ensure that you set up the required permission configuration for S3, Glue and EMR.
One or more settings might be incorrectly configured in the Segment app, preventing your Data Lakes syncs from succeeding.
If you have invalid settings, you might see one of the error messages below:
The most common error occurs when you do not list all Source IDs in the External ID section of the IAM role. You can find your Source IDs in the Segment workspace, and you must add each one to the list ofExternal IDsin the IAM policy. You can either update the IAM policy from the AWS Console, or re-run theData Lakes set up Terraform job.
Internal errors occur in Segment’s internal systems, and should resolve on their own. If sync failures persist,contact the Segment Support team.
Both Warehouses and Data Lakes provide similar information about syncs, including the start and finish time, rows synced, and errors.
However, Warehouse sync information is only available in the Segment app: on the Sync History page and Warehouse Health pages. With Data Lakes sync reports, the raw sync information is sent directly to your data lake. This means you can query the raw data and answer your own questions about syncs, and use the data to power alerting and monitoring tools.
Sync reports are currently generated only when a sync completes, or when it fails. Partial failure reporting is not currently supported.

Headings:
Data Lakes Sync Reports and Errors
Sync Report schema
Data location
Data format
Querying the Sync Reports table
Return row counts per day for a specific event
Return row counts per day for all events in the source
Find the most recent successful sync
Find all failures in the last N days
Sync errors
Insufficient permissions
Invalid settings
Internal error
FAQ
How are Data Lakes sync reports different from the sync data for Segment Warehouses?
What happens if a sync is partly successful?
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/data-lakes/lake-formation/
Paragraphs:
Data Lakes is available for the listed account plans only.See theavailable plans, orcontact Support.
On this page
Lake Formation is a fully managed service built on top of the AWS Glue Data Catalog that provides one central set of tools to build and manage a Data Lake. These tools help import, catalog, transform, and deduplicate data, as well as provide strategies to optimize data storage and security. To learn more about Lake Formation features, seeAmazon Web Services documentation.
The security policies in Lake Formation use two layers of permissions: each resource is protected by Lake Formation permissions (which control access to Data Catalog resources and S3 locations) and IAM permissions (which control access to Lake Formation and AWS Glue API resources). When any user or role reads or writes to a resource, that action must pass a both a Lake Formation and an IAM resource check: for example, a user trying to create a new table in the Data Catalog may have Lake Formation access to the Data Catalog, but if they don’t have the correct Glue API permissions, they will be unable to create the table.
For more information about security practices in Lake Formation, see Amazon’sLake Formation Permissions Referencedocumentation.
You can configure Lake Formation using theIAMAllowedPrincipalsgroupor byusing IAM policies for access control. Configuring Lake Formation using theIAMAllowedPrincipalsgroup is an easier method, recommended for those exploring Lake Formation. Setting up Lake Formation using IAM policies for access control is a more advanced setup option, recommended for those who want additional customization options.
Permissions required to configure Data Lakes
To configure Lake Formation, you must be logged in to AWS with data lake administrator or database creator permissions.
To verify that you’ve configured Lake Formation, open theAWS Lake Formation service, selectData lake permissions, and verify theIAMAllowedPrincipalsgroup is listed with “All” permissions.
Granting Super permission to IAM roles
If you manually configured your database, assign theEMR_EC2_DefaultRoleSuper permissions in step 8. If you configured your database using Terraform, assign thesegment_emr_instance_profileSuper permissions in step 8.

Headings:
Lake Formation
Configure Lake Formation
Configure Lake Formation using the IAMAllowedPrincipals group
Existing databases
New databases
Verify your configuration
Configure Lake Formation using IAM policies
Existing databases
New databases
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/data-lakes/sync-history/
Paragraphs:
Data Lakes is available for the listed account plans only.See theavailable plans, orcontact Support.
On this page
The Segment Data Lakes sync history and health tabs generate real-time information about data syncs so you can monitor the health and performance of your data lakes. These tools provide monitoring and debugging capabilities within the Data Lakes UI, so you can identify and proactively address data sync or data pipeline failures.
The Sync History table shows detailed information about the latest 100 syncs to the data lake. The table includes the following fields:
Selecting a row in the Sync History table opens a sidebar showing the number of rows from each collection that synced.
To access the Sync History page from the Segment app, open theMy Destinationspage and select the data lake. On the data lakes Settings page, select theSync Historytab.
The health tab provides an overview of the rows that synced to your data lake both today and each day for the last 30 days.
The bar chart, ‘Daily Synced Rows,’ shows an overview of the rows synced for each of the last 30 days. Hovering over a date shows the number of rows that were synced for that day. Selecting a date from the bar chart opens the Daily Row Volume table, which provides a breakdown of which collections synced, how many rows from each collection synced, and the percentage of all synced rows from each collection.
The Daily Row Volume table contains the following information:
Above the Daily Row Volume table is an overview of the total syncs for the current day, showing the number of rows synced, the number of collections that synced, and the current date.
To access the Sync history page from the Segment app, open theMy Destinationspage and select the data lake. On the data lakes settings page, select theHealthtab.
The health tab shows an aggregate view of the last 30 days worth of data, while the sync history retains the last 100 syncs.
The sync history feature shows detailed information about the most recent 100 syncs to a data lake, while the health tab shows just the number of rows synced to the data lake over the last 30 days.
All dates and times on the sync history and health pages are in the user’s local time.
The sync data for both reports updates in real time.
Syncs occur approximately every two hours. Users cannot choose how frequently the data lake syncs.

Headings:
Data Lakes Sync History and Health
Sync history
Health
Data Lakes reports FAQ
How long is a data point available?
How do sync history and health compare?
What timezone is the time and date information in?
When does the data update?
When do syncs occur?
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/data-lakes/comparison/
Paragraphs:
Data Lakes is available for the listed account plans only.See theavailable plans, orcontact Support.
On this page
As Segment builds new data storage products, each product evolves from prior products to best support the needs of customers. Segment Data Lakes is an evolution of the Warehouses product that meets the changing needs of customers.
Data Lakes and Warehouses are not identical, but are compatible with a configurable mapping. This mapping helps you to identify and manage the differences between the two storage solutions, so you can easily understand how the data in each is related.
Data Lakes and Warehouses offer different sync frequencies:
Segment’s99% guarantee of no duplicatesfor data within a 24 hour look-back window applies to data in Segment Data Lakes and Warehouses.
WarehousesandData Lakesalso have a secondary deduplication system to further reduce the volume of duplicates to ensure clean data in your Warehouses and Data Lakes.
Warehouses support both event and object data, while Data Lakes supports only event data.
See the table below for information about thesourcetypes supported by Warehouses and Data Lakes.
Warehouses and Data Lakes both infer data types for the events each receives. Since events are received by Warehouses one by one, Warehouses look at the first event received every hour to infer the data type for subsequent events. Data Lakes uses a similar approach, however because it receives data every hour, Data Lakes is able to look at a group of events to infer the data type.
This approach leads to a few scenarios where the data type for an event may be different between Warehouses and Data Lakes. Those scenarios are:
Variance in data types between Warehouses and Data Lakes don’t happen often for booleans, strings, and timestamps, however it can occur for decimals and integers.
If a bad data type is seen, such as text in place of a number or an incorrectly formatted date, Warehouses and Data Lakes attempt a best effort conversion to cast the fields to the target data type. Fields that cannot be casted may be dropped.Contact Segment Supportif you want to correct data types in the schema and perform areplayto ensure no data is lost.
Tables between Warehouses and Data Lakes will be the same, except for in these two cases:
Similar to tables, columns between Warehouses and Data Lakes will be the same, except for in a few specific scenarios:

Headings:
Comparing Data Lakes and Warehouses
Data freshness
Duplicates
Object vs event data
Schema
Data types
Tables
Columns
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/warehouses/
Paragraphs:
Free and Team plan workspaces can have 1 warehouse. Business plans can have more than one, and include custom sync schedules and filtering.See theavailable plans, orcontact Support.
On this page
A warehouse is a central repository of data collected from one or more sources. This is what commonly comes to mind when you think about a relational database: structured data that fits neatly into rows and columns.
In Segment, a Warehouse is a special type of destination. Instead of streaming data to the destination all the time, we load data to them in bulk at regular intervals. When we load data, we insert and update events and objects, and automatically adjust their schema to fit the data you’ve sent to Segment.
When selecting and building a data warehouse, consider three questions:
Relational databases are great when you know and predefine the information collected and how it will be linked. This is usually the type of database used in the world of user analytics. For instance, a users table might be populated with the columnsname,email_address, orplan_name.
Examples of data warehouses include Amazon Redshift, Google BigQuery, and Postgres.
When Segment loads data into your warehouse, each sync goes through two steps:
Looking for the Warehouse Schemas docs?
They’ve moved:Warehouse Schemas.
When your existing analytics tools can't answer your questions, it's time to level-up and use SQL for analysis.
How do I send custom data to my warehouse?
How do I give users permissions to my warehouse?
Check out theFrequently Asked Questions about Warehousespage anda list of helpful SQL queries to get you started with Redshift.
How do I decide between Redshift, Postgres, and BigQuery?
What do you recommend for Postgres: Amazon or Heroku?
How do I give users permissions?
What are the limitations of Redshift clusters and warehouses connectors?
Where do I find my source slug?
How do I create a user, grant usage on a schema and then grant the privileges that the user will need to interact with that schema?
Which IPs should I allowlist?
Will Segment sync my historical data?
Can I load in my own data into my warehouse?
Can I control what data is sent to my warehouse?
How fresh is the data in my warehouse?
Can I add, tweak, or delete some of the tables?
Can I transform or clean up old data to new formats or specs?
What are common errors and how do I debug them?
How do I speed up my Redshift queries?
How do I forecast LTV with SQL and Excel for e-commerce businesses?
How do I measure the ROI of my Marketing Campaigns?

Headings:
Data Warehouses
What’s a Warehouse?
Analytics Academy: When to use SQL for analysis
More Help
FAQs
Setting up a warehouse
Managing a warehouse
Analyzing with SQL
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/warehouses/schema/
Paragraphs:
On this page
Aschemadescribes the way that the data in a warehouse is organized. Segment stores data in relational schemas, which organize data into the following template:<source>.<collection>.<property>, for examplesegment_engineering.tracks.user_id, where source refers to the source or project name (segment_engineering), collection refers to the event (tracks), and the property refers to the data being collected (user_id). All schemas convert collection and property names fromCamelCasetosnake_caseusing thego-snakecasepackage.
Warehouse column creation
Note:Segment creates tables for each of your custom events in your warehouse, with columns for each event’s custom properties. Segment does not allow unboundedeventorpropertyspaces in your data. Instead of recording events like “Ordered Product 15”, use a single property of “Product Number” or similar.
Segment creates and populates a column only when it receives a non-null value from the source.
Segment’s libraries pass nested objects and arrays into tracking calls asproperties,traits, andtracking calls. To preserve the quality of your events data, Segment uses the following methods to store properties and traits in database tables:
The table below describes the schema in Segment Warehouses:
Theidentifiestable stores the.identify()method calls. Query it to find out user-level information. It has the following columns:
To see a list of the columns in theidentifiestable for your<source>, run the following:
Theidentifiestable is where you can query information about your users and their traits. For example, this query returns unique users you’ve seen on your site each day:
Thegroupstable stores thegroupmethod calls. Query it to find out group-level information. It has the following columns:
To see a list of the columns in thegroupstable for your<source>, run the following:
To see a list of the groups using your product, run the following:
Thepagesandscreenstables store thepageandscreenmethod calls. Query it to find out information about page views or screen views. It has the following columns:
To see a list of the columns in thepagestable for your<source>, run the following:
The pages table can give you interesting information about page views that happen on your site. The following query, for example, shows page views grouped by day:
Thetrackstable stores thetrackmethod calls. Query it to find out information about the events your users have triggered. It has the following columns:
Yourtrackstable is a rollup of the different event-specific tables, for quick querying of just a single type. For example, you could see the number of unique users signed up each day:
Your event tables are a series of table for each custom event you record to Segment. We break them out into their own tables because the properties, and, as a result, the columns, differ for each event. Query these tables to find out information about specific properties of your custom events. They have the following columns:
To see a list of the event tables for a given<source>, run the following:
To see a list of the columns in one of your event tables, run the following:
To see the tables for your organization, you can run this query:
Thesource.eventtables have the same columns as thesource.tracktables, but they also include columns specific to the properties of each event.
If you’re recording an event like:
Then you can expect to see columns namedplanandaccount_typeas well as the defaultevent,id, and so on. That way, you can write queries against any of the custom data sent in track calls.
Note
Because Segment addspropertiesandtraitsas un-prefixed columns to your tables, there is a chance the names can collide with the reserved column names. For this reason, Segment discards properties with the same name as the reserved column name (for example,user_id).
Your event tables are one of the more powerful datasets in Segment SQL. They allow you to see which actions users perform when interacting with your product.
Because every source has different events, what you can do with them will vary. Here’s an example where you can see the number of “Enterprise” users signed up for each day:
Here’s an example that queries the daily revenue for an ecommerce store:
New event properties and traits create columns. Segment processes the incoming data in batches, based on either data size or an interval of time. If the table doesn’t exist we lock and create the table. If the table exists but new columns need to be created, we perform a diff and alter the table to append new columns.
When Segment process a new batch and discover a new column to add, we take the most recent occurrence of a column and choose its datatype.
The data types that Segment currently supports include:
Data types are set up in your warehouse based on the first value that comes in from a source. For example, if the first value that came in from a source was a string, Segment would set the data type in the warehouse tostring.
In cases where a data type is determined incorrectly, the support team can help you update the data type. As an example, if a field can include float values as well as integers, but the first value we received was an integer, we will set the data type of the field to integer, resulting in a loss of precision.
To update the data type, reach out to the Segment support team. They will update the internal schema that Segment uses to infer your warehouse schema. Once the change is made, Segment will start syncing the data with the correct data type. However, if you want to backfill the historical data , you must drop the impacted tables on your end so that Segment can recreate them and backfill those tables.
To request data types changes, please reach out toSegment Supportfor assistance, and provide with these details for the affected columns in the following format:<schema_name>.<table_name>.<column_name>.<current_datatype>.<new_datatype>
After analyzing the data from dozens of customers, we set the string column length limit at 512 characters. Longer strings are truncated. We found this was the sweet spot for good performance and ignoring non-useful data.
Segment uses special-case compression for some known columns, like event names and timestamps. The others default to LZO. Segment may add look-ahead sampling down the road, but from inspecting the datasets today this would be unnecessarily complex.
The Segment API associates four timestamps with every call:timestamp,original_timestamp,sent_atandreceived_at.
All four timestamps pass through to your Warehouse for every ETL’d event. In most cases the timestamps are close together, but they have different meanings which are important.
timestampis the UTC-converted timestamp which is set by the Segment library. If you are importing historical events using a server-side library, this is the timestamp you’ll want to reference in your queries.
original_timestampis the original timestamp set by the Segment library at the time the event is created.  Keep in mind, this timestamp can be affected by device clock skew. You can override this value by manually passing in a value fortimestampwhich will then be relabeled asoriginal_timestamp. Generally, this timestamp should be ignored in favor of thetimestampcolumn.
sent_atis the UTC timestamp set by library when the Segment API call was sent.  This timestamp can also be affected by device clock skew.
received_atis UTC timestamp set by the Segment API when the API receives the payload from client or server. All tables usereceived_atfor the sort key.
Segment recommends using thereceived_attimestamp for all queries based on time. The reason for this is two-fold. First, thesent_attimestamp relies on a client’s device clock being accurate, which is generally unreliable. Secondly, Segment setsreceived_atas the sort key in Redshift schemas, which means queries will execute much faster when usingreceived_at. You can continue to usetimestamporsent_attimestamps in queries ifreceived_atdoesn’t work for your analysis, but the queries will take longer to complete.
For Business Tier customers, Segment suggests enablingreceived_atin the Selective Sync settings to ensure syncs and backfills complete successfully.
received_atdoes not ensure chronology of events.  For queries based on event chronology,timestampshould be used.
ISO-8601 date strings with timezones included are required when using timestamps withEngage. Sending custom traits without a timezone included in the timestamp will result in the value not being saved.
To learn more about timestamps in Segment,read our timestamps overviewin the Segment Spec.
Each row in your database will have anidwhich is equivalent to the messageId which is passed through in the raw JSON events. Theidis a unique message id associated with the row.
Theuuidcolumn is used to prevent duplicates. You can ignore this column.
Theuuid_tscolumn is used to keep track of when the specific event was last processed by our connector, specifically for deduping and debugging purposes. You can generally ignore this column.
Theloaded_atcolumn contains the UTC timestamp reflecting when the data was staged by the processor. This column is created only in BigQuery warehouse.
All tables usereceived_atfor the sort key. Amazon Redshift stores your data on disk in sorted order according to the sort key. The Redshift query optimizer uses sort order when it determines optimal query plans.
How do I send custom data to my warehouse?
How do I give users permissions to my warehouse?
How frequently does data sync to my warehouse?
Check out ourFrequently Asked Questions about Warehousesanda list of helpful Redshift queries to get you started.

Headings:
Warehouse Schemas
How warehouse tables handle nested objects and arrays
Warehouse tables
Identifies table
Querying the Identifies table
Groups table
Querying the Groups table
Pages and Screens tables
Querying the Pages and Screens tables
Tracks table
Querying the Tracks table
Event Tables
Querying the Events tables
Tracks vs. Events Tables
Schema Evolution and Compatibility
New Columns
Data Types
timestamp
integer
float
boolean
varchar
Column Sizing
Timestamps
id
uuid, uuid_ts, and loaded_at
Sort Key
More Help
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/warehouses/warehouse-syncs/
Paragraphs:
On this page
Instead of constantly streaming data to the warehouse destination, Segment loads data to the warehouse in bulk at regular intervals. Before the data loads, Segment inserts and updates events and objects, and automatically adjusts the schema to make sure the data in the warehouse is inline with the data in Segment.
When Segment loads data into your warehouse, each sync goes through two steps:
Warehouses sync with all data coming from your source. However, Business plan members can manage the data that is sent to their warehouses usingSelective Sync.
Your plan determines how frequently data is synced to your warehouse.
*If you’re a Business plan member and would like to adjust your sync frequency, you can do so using the Selective Sync feature. To enable Selective Sync, please go toWarehouse>Settings>Sync Schedule.
Why can't I sync more than 24 times per day?
We do not set syncs to happen more than once per hour (24 times per day). The warehouse product is not designed for real-time data, so more frequent syncs would not necessarily be helpful.
You can use the Sync History page to see the status and history of data updates in your warehouse. The Sync History page is available for every source connected to each warehouse. This page helps you answer questions like, “Has the data from a specific source been updated recently?” “Did a sync completely fail, or only partially fail?” and “Why wasn’t this sync successful?”
The Sync History includes the following information:
If a sync run shows a partial success or failure, the next sync attempts to sync any data that was not successfully synced in the prior run.
To view the Sync History:
Warehouse Selective Sync allows you to manage the data that you send to your warehouses. You can use this feature to stop syncing specific events (also known as collections) or properties that aren’t relevant, and may slow down your warehouse syncs.
This feature is only available to Business Tier customers.You must be a Workspace Owner to change Selective Sync settings.
With Selective Sync, you can customize which collections and properties from a source are sent to each warehouse. This helps you control the data that is sent to each warehouse, allowing you to sync different sets of data from the same source to different warehouses.
NOTE:This feature only affectswarehouses, and doesn’t prevent data from going to any otherdestinations.
When you disable a source, collection or property, Segment no longer syncs data from that source. Segment won’t delete any historical data from your warehouse. When you re-enable a source, Segment syncs all events since the last sync. This doesn’t apply when a collection or property is re-enabled. Only new data generated after re-enabling a collection or property will sync to your warehouse.
For each warehouse only the first 5,000 collections per source and 5,000 properties per collection are visible in the Selective Sync user interface.Learn more about the limits.
Disabling thereceived_atcolumn will cause your syncs to fail, as all tables usereceived_atas the sort key.
By default, all sources and their collections and properties are sent, and no data is prevented from reaching warehouses.
When you disable sources, collections, or properties using Selective Sync, Segment stops sending new data for these sources, collections, or properties to your warehouse. It doesn’t delete any existing data in the warehouse.
If you choose to re-enable a source to begin syncing again, Segment loads all data that arrived since the last sync into the warehouse, but doesn’t backfill data that was omitted while these were disabled. When a collection or property is re-enabled, data only syncs going forward. It will not be loaded from the last sync.
To use Selective Sync:
To change the sync settings to a single warehouse from multiple sources, follow the same steps asabove.
This may be valuable if you’re looking to make changes in bulk, such as when setting up a new warehouse.
To manage data from one specific source to an individual warehouse:
This may be valuable when you’re making smaller changes, for example, disabling all properties from one unnecessary collection.
All changes made through Selective Sync only impact an individual warehouse. They don’t impact multiple warehouses at once. To make changes to multiple warehouses, you need to enable/disable data for each individual warehouse.
Regardless of schema size, for each warehouse only the first 5,000 collections per source and 5,000 properties per collection can be managed using the Selective Sync user interface. After you hit any of these limits, all future data is still tracked and sent to your warehouse. New collections created after hitting this limit is not displayed in the Selective Sync table.
You will see a warning in the Selective Sync user interface when the warehouse schema has reached 80% of the limit for collections and/or properties. An error message will appear when you’ve reached the limit.
ContactSupportto edit Selective Sync settings for any collections and/or properties which exceed the limit.
Only Workspace Owners can change Selective Sync settings.

Headings:
Warehouse Syncs
Sync Frequency
Sync History
View the Sync History
Warehouse Selective Sync
When to use Selective Sync
Enable Selective Sync
Change sync settings to a single warehouse from multiple sources
Change sync settings on a specific Warehouse to Source connection
Selective Sync User Interface Limits
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/warehouses/health/
Paragraphs:
On this page
The Warehouse Health dashboard helps you understand trends in data volume (specifically, rows) synced to your data warehouse over time.
You can use this feature to answer questions such as:
Note: Warehouse Health is available for all Warehouse customers.
The Warehouse Health dashboards are available at both thewarehouse level, and at thewarehouse-source connection level, explained below.
Data in the dashboards updates in real-time, and covers the previous 30 days. The timezones displayed in the dashboards are converted to the viewer’s local time.
Go to the Segment App, to the Destinations list, and select the warehouse. On the warehouse’s information page, click theHealthtab.
This dashboard displays aggregate trends fromallsources that sync to the specific warehouse.
A warehouse level dashboard
Go to the Segment App, to the Destinations list, and select the warehouse. On the warehouse’s Overview page, select the Source (schema) you want to see data for, then click theHealthtab.
This dashboard displays trends for each separate source that syncs to a specific warehouse. It also displays aggregations of the collections within that source.
A warehouse-source level dashboard
No. These dashboards exist to help you understand high-level trends, but not to provide exact numbers about the data synced to the warehouse. The numbers provided in these dashboards are rounded, and are not exact.
These dashboards will help you understand trends in the data, and use signals to do deeper investigation and QA, as needed.
The Warehouse Overview, Sync History and Health tabs provide different levels of granularity into warehouse syncs.
Data is refreshed on a real time basis.

Headings:
Warehouse Health Dashboard
Warehouse dashboards
Warehouse-Source dashboards
Warehouse Health Dashboard FAQs
Can I use the Health Dashboard data for QA and validation?
How is this similar (or different) than the information available in the Sync History and Overview tabs?
How often is the data refreshed?
What timeframe is the data available for?
What timezone are the dates in?
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/warehouses/choose-warehouse/
Paragraphs:
On this page
In most cases, you will get a much better price-to-performance ratio with Redshift for typical analyses.
Redshift lacks somefeatures,datatypes, andfunctionssupported by Postgres and also implementssome featuresdifferently. If you need any of the features or functions missing in Redshift and BigQuery, choose Postgres. If not (or you’re not sure), Segment recommends choosing Redshift.
If you’d like more information, Amazon wroteabout this in their documentation.
Both Redshift and BigQuery are attractive cloud-hosted, affordable, and performant analytical databases. The differences between the two are around their architecture and pricing.
When you provision a Redshift cluster, you’re renting a server from Amazon Web Services. Your cluster consists ofnodes, each with dedicated memory, CPU, and disk storage. These nodes handle data storage, query execution, and - if your cluster contains multiple nodes - a leader node will handle coordination across the cluster.
Redshift performance and storage capacity is a function of cluster size and cluster type. As your storage or performance requirements change, you can scale up or down your cluster as needed.
With BigQuery, you’re not constrained by the storage capacity or compute resources of a given cluster. Instead, you can load large amounts of data into BigQuery without running out of memory, and execute complex queries without maxing out CPU.
This is possible because BigQuery takes advantage of distributed storage and networking to separate data storage from compute power. Google’sColossus distributed file systemdistributes data across many servers in the Google cloud. When you execute a query, theDremel query enginesplits the query into smaller sub-tasks, distributes the sub-tasks to computers across Google data centers, and then re-assembles them into your results.
The difference in architecture translates into differences in pricing.
Redshift pricesare based on an hourly rate determined by the number and types of nodes in your cluster. They offer dense storage - optimized for storage - and dense compute nodes - optimized for query performance.
BigQuery has twopricing options: variable and fixed pricing. With the variable, pay-as-you-go plan, you pay for the data you load into BigQuery, and then pay for the amount of data you query. BigQuery allows you to set upCost Controls and Alertsto help control and monitor costs.
Fixed-price plans are more for high-volume customers and allow you to rent a fixed amount of compute power.
Redshift does require you to create a cluster, choose sort and distribution keys, and resize your cluster as storage and performance needs change over time.
BigQuery is “fully-managed”, which means that you’ll never have to resize or adjust distribution or sort keys. BigQuery handles all of that.

Headings:
Choosing a Warehouse
Comparing Redshift and Postgres
Comparing Redshift and BigQuery
Architecture
Pricing
Resource Management
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/warehouses/faq/
Paragraphs:
On this page
Yes. Customers on Segment’sBusiness plancan choose which sources, collections, and properties sync to your data warehouse usingWarehouse Selective Sync.
Selective Sync helps manage the data Segment sends to each warehouse, allowing you to sync different sets of data from the same source to different warehouses.
When you disable a source, collection or property, Segment no longer syncs data from that source. Segment won’t delete any historical data from your warehouse. When you re-enable a source, Segment syncs all events since the last sync. This doesn’t apply when a collection or property is re-enabled. Only new data generated after re-enabling a collection or property will sync to your warehouse.
You can also use theIntegration Objectto control whether or not data is sent to a specific warehouse.
You have full admin access to your Segment Warehouse. However, don’t tweak or delete Segment generated tables, as this may cause problems for the systems that upload new data.
If you want to join across additional datasets, feel free to create and upload additional tables.
This is a common question if the data you’re collecting has evolved over time. For example, if you used to track the eventSignupbut now trackSigned Up, you’d probably like to merge those two tables to make querying simple and understandable.
Segment does not have a way to update the event data in the context of your warehouse to retroactively merge the tables created from changed events. Instead, you can create a “materialized” view of the unioned events. This is supported inRedshift,Postgres,Snowflake, and others, but may not be available inallwarehouses.
Protocols customers can also useTransformationsto change events at the source, which applies to all cloud-mode destinations (destinations that receive data from the Segment servers)includingyour data warehouse. Protocols Transformations offer an excellent way to quickly resolve implementation mistakes and help transition events to a Segment spec.
Note: Transformations are currently limited to event, property and trait name changes, and donotapply to historical data.
Yes. Data types are initially set up in your warehouse based on the first value that comes in from a source, but you can request data type changes by reaching out toSegment supportfor assistance.
Keep in mind that Segment only usesgeneral data typeswhen loading data in your warehouse. Therefore, some of the common scenarios are:
More granular changes (such as the examples below) wouldn’t normally be handled by the Support team, thus they often need to be made within the warehouse itself:
The data type definitions in Protocols have no impact on the warehouse schema.
Your source slug can be found in the URL when you’re looking at the source destinations page or live debugger. The URL structure will look like this:
https://segment.com/[my-workspace]/sources/[my-source-slug]/overview
Your warehouse id appears in the URL when you look at thewarehouse destinations page. The URL structure looks like this:
app.segment.com/[my-workspace]/warehouses/[my-warehouse-id]/overview
Data is available in Warehouses within 24-48 hours, depending on your tier’s sync frequency. For more information about sync frequency by tier, seeSync Frequency.
Real-time loading of the data into Segment Warehouses would cause significant performance degradation at query time. To optimize for your query speed, reliability, and robustness, Segment guarantees that your data will be available in your warehouse within 24 hours. The underlying datastore has a subtle tradeoff between data freshness, robustness, and query speed. For the best experience, Segment needs to balance all three of these.
You can freely load data into your Segment Warehouse to join against your source data tables.
The only restriction when loading your own data into your connected warehouse is that you should not add or remove tables within schemas generated by Segment for your sources. Those tables have a naming scheme of<source-slug>.<table>and should only be modified by Segment. Arbitrarily deleting columns from these tables may result in mismatches upon load.
If you want to insert custom data into your warehouse, create new schemas that are not associated with an existing source, since these may be deleted upon a reload of the Segment data in the cluster.
Segment recommends scripting any sort of additions of data you might have to warehouse, so that you aren’t doing one-off tasks that can be hard to recover from in the future in the case of hardware failure.
Segment recommends enabling IP allowlists for added security. All Segment users with workspaces hosted in the US who use allowlists in their warehouses must update those allowlists to include the following ranges:
Users with workspaces in the EU must allowlist3.251.148.96/29.
Segment loads up to two months of your historical data when you connect a warehouse.
For full historical backfills you’ll need to be a Segment Business plan customer. If you’d like to learn more about our Business plan and all the features that come with it,check out our pricing page.
Heroku’s simple set up and administration process make it a great option to get up and running quickly.
Amazon’s service has some more powerful features and will be more cost-effective for most cases. However, first time users of Amazon Web Services (AWS) will likely need to spend some time with the documentation to get set up properly.
When you create a new source, the source syncs to all warehouse(s) in the workspace by default. You can prevent the source from syncing to some or all warehouses in the workspace in two ways:
After a source is created, you can enable or disable a warehouse sync within the Warehouse Settings page.
If you enabled activity notifications for your storage destination, you’ll receive notifications in the Segment app for the fifth and 20th consecutive warehouse failures for all incoming data. Segment does not track failures on a per connection (‘source<>warehouse’) basis. Segment’s notification structure also identifies global issues encountered when connecting to your warehouse, like bad credentials or being completely inaccessible to Segment.
To sign up for warehouse sync notifications:
Data in your warehouse is formatted intoschemas, which involve a detailed description of database elements (tables, views, indexes, synonyms, etc.)
and the relationships that exist between elements. Segment’s schemas use the following template:<source>.<collection>.<property>, for example,segment_engineering.tracks.user_id, where source refers to the source or project name (segment_engineering), collection refers to the event (tracks),
and the property refers to the data being collected (user_id).Note:It is not possible to have different sources feed data into the same schema in your warehouse. While setting up a new schema, you cannot use a duplicate schema name.
Schema data for Segment warehouses is represented in snake case.
For more information about Warehouse Schemas, see theWarehouse Schemaspage.
If your syncs fail, you do not need to reach out to Segment Support to request a backfill. Once a successful sync takes place,
Segment automatically loads all of the data generated since the last successful sync occurred.
Segment stores the name of your schema in theSQL Settingspage. Changing the name of your schema in the app without updating the name in your data warehouse causes a new schema to form, one that doesn’t contain historical data.
To change the name of your schema without disruptions:
Note: This will set the schema name for all existing and future destinations. The new name must be lowercase and may include underscores.
At the moment, there isn’t a way to selectively filter events that are sent to the warehouse. The warehouse connector works quite differently from our streaming destinations and only has theselective syncfunctionality that allows you to enable/disable specific properties or events.
It’s not possible for different sources to sync data directly to the same schema in your warehouse. When setting up a new schema within the Segment UI, you can’t use a schema name that’s already in use by another source. Segment recommends syncing the data separately and then joining it downstream in your warehouse.
For more information about Warehouse Schemas, see theWarehouse Schemaspage.

Headings:
Warehouse FAQs
Can I control what data is sent to my warehouse?
Don’t send data to any Warehouse
Send data to all Warehouses
Send data to specific Warehouses
Can we add, tweak, or delete some of the tables?
Can we transform or clean up old data to new formats or specs?
Can I change the data type of a column in the warehouse?
Can the data type definitions in Protocols be enforced in a warehouse schema?
How do I find my source slug?
How do I find my warehouse id?
How fresh is the data in Segment Warehouses?
What if I want to add custom data to my warehouse?
Which IPs should I allowlist?
Will Segment sync my historical data?
What do you recommend for Postgres: Amazon or Heroku?
How do I prevent a source from syncing to some or all warehouses?
Can I be notified when warehouse syncs fail?
How is the data formatted in my warehouse?
If my syncs fail and get fixed, do I need to ask for a backfill?
Can I change my schema names once they’ve been created?
Can I selectively filter data/events sent to my warehouse based on a property?
Can data from multiple sources be synced to the same database schema?
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/warehouses/add-warehouse-users/
Paragraphs:
If you have more than one person working with your Segment Warehouse, you might want to create users for your team so that each person can have a discrete login. The three steps in this section will show you how to create a user, grant usage on a schema and then grant the privileges that the user will need to interact with that schema.
The code above in [] is optional, you don’t need to group your users or give their credentials an expiration date, the code works without it. However, if you do choose to use those parameters,<abstime>should be formatted as ‘2015-09-13’ which translates to September 13th, 2015.
For instance, you can create a user namedflashtheslothas
This creates a user, you can run the following to get a list of users in your database.
Now that we’ve confirmed that the user has been created, they already have access to the public schema that contains systems-level information about the cluster but we need to give them access to the specific schemas that they’ll be working in.
Next,GRANT USAGEon the schema to the user we just created
The above SQL command grants the user USAGE privileges on a schema. Let’s assume you want to grantflashtheslothaccess to your development schema, it would look like below
Our new user now has usage rights on thedevelopmentschema, now we need to grant the type of SQL commands they’ll be able to run against the cluster. For the purposes of this example, we’re going to give the user read only privileges.
GRANT SELECTprivileges so the user can query the tables
The above SQL command grants the user SELECT rights on all tables in the chosen schema. For ourflashtheslothuser and thedevelopmentschema, it would look like below.
Doing these three steps will result in a new user that can query all the tables in a given schema. If you want to give access to more than one schema then you can simply repeat steps 2 and 3 for each additional schema. If you have any questions or if you’re running into any issues getting this set up,contact us.

Headings:
Adding Warehouse Users
1. Creating a user with theCREATE USERcommand
2. Grant usage on the schema
3. Grant select privileges
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/warehouses/warehouse-errors/
Paragraphs:
This is a permissioning issue. To learn how to set up proper permissions, you can check out ourPostgresandRedshiftguides.
This error occurs when your cluster is currently resizing. The warehouse will continue on its scheduled run interval - once the resize is complete, we’ll load all data from the failed run.
This is a Redshift 500 - an internal server error. This is usually caused by having too many tables or too many columns. If you’re seeing this error,contact the Segment Support team.
This is a networking error that typically arises when Redshift doesn’t close properly close the connection or gets rebooted.
If you see this error on consecutive syncs,contact us.
This is a permissioning issue. To learn how to set up proper permissions, you can check out ourpostgresandredshiftguides.
This is a credential issue. To fix your credentials, head over to Warehouse > Settings > Connection.
This is a credential issue. To fix your credentials, head over to Warehouse > Settings > Connection.
This is a networking error. The connection times out because Redshift doesn’t close properly or gets rebooted.
If you see this error on consecutive syncs,contact us.
This occurs when a Postgres database is incorrectly connected as Redshift. To resolve this, delete the warehouse and reconnect, using the Postgres set up option.
This error is generally a network error when Redshift closes the connection. If the problem persists on multiple runs,contact us.
This is a permissioning issue. To learn how to set up proper permissions, you can check out ourpostgresandredshiftguides.
This error is generally a network error when Redshift closes the connection. If the problem persists on multiple runs,contact us.
This error indicates that a column that is attempting to sync has the same title as a reserved keyword in Snowflake. More information regarding Snowflake’s reserved keywords can be foundin Snowflake’s docs.

Headings:
Warehouse Errors
ERROR: Schema “XXX” does not exist. (SQLSTATE 3F000)
ERROR: Cannot execute query because system is in resize mode (SQLSTATE 57014)
ERROR: 1040 (SQLSTATE XX000)
read tcp XXX.XX.XX.XXXX:XXXX-XXX.XX.XX.XXXX:XXXX: read: connection timed out
pq: role “XXX” is not permitted to log in
pq: password authentication failed for user “XXX”;
dial tcp: lookup XXX-hostname on 10.50.0.2:53: no such host
dial tcp XX.XXX.XXX.XXX:XXXX: getsockopt: connection timed out / refused
ERROR: syntax error at or near “ENCODE”; (SQLSTATE 42601)
Error during deduping step for collectionXXX: EOF
ERROR: permission denied for relation update (SQLSTATE 42501)
EOF
ERROR: failed to create table: 002318 (42601): SQL compilation error: invalid column definition name “XXX” (ANSI reserved)
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/warehouses/redshift-faq/
Paragraphs:
On this page
“Are there limitations of Redshift clusters and our Redshift connector?”
While Redshift clusters are incredibly scalable and efficient, limitations are imposed to ensure that clusters maintain performance.
Redshift does not allow you to create tables or columns using reserved words. To avoid naming convention issues, we prepend a_to any reserved word names. If you’re having trouble finding a column or table, you can check the list ofRedshift reserved wordsor search for the table with a prepended underscore like_open.
Redshift sets the maximum number of tables you can create in a cluster to 9,900 including temporary tables. While it’s rare to reach that limit, we recommend keeping an eye on the number of tables our warehouse connector is creating in your cluster. Keep in mind that a new table is created for each unique event you send to Segment, which becomes an issue if events are being dynamically generated.
When setting up your Redshift cluster, you can select between dense storage (ds2) and dense compute (dc1) cluster types. Dense compute nodes are SSD based which allocates only 200GB per node, but results in faster queries. Dense storage nodes are hard disk based which allocates 2TB of space per node, but result in slower queries. When scaling up your cluster by adding nodes, it’s important to remember that adding more nodes will not add space linearly. As you add more dc1 nodes, the amount of preallocated space for each table increases. For example, if you have a table with 10 columns, Redshift will preallocate 20mb of space (10 columns X 2 slices) per node. That means that the same table will preallocate 20mb of space in a single ds2 cluster, and 200mb in a 10 node dc1 cluster.
Like with most data warehouses, column data types (string, integer, float, etc.) must be defined at the time the column is created. Unlike most data warehouses, Redshift does not allow for easy column type changes after the column has been created. Additionally, we store a record of what the tables and column types should be set to in a local database, and validate the structure on each connector run. Currently, column type changes (i.e. change an integer column to float) are only available to our business tier customers on an ad-hoc basis.
All Segment-managed schemas have a default VARCHAR size of 512 in order to keep performance high. If you wish to increase the VARCHAR size, you can run the following query.
Example:
Increasing the default size can impact query performance as it needs to process more data to accomodate the increased column size. SeeAmazon’s Redshift Documentationfor more details.
While almost all event properties are valid, we are unable to pass through properties that have naming conflicts with the default key/value pairs included in a standard raw JSON call. For example, if you send through a property in a track call named “timestamp” or “event”, it will cause a conflict and you likely wont see it appear in your warehouse. To be more specific, if you send the following track call, {‘event’:’birthday’} will likely be dropped when syncing the data to your data warehouse.
analytics.track('selected gift', {'event':'birthday', 'type':'cake'})
This page was last modified: 11 Mar 2021
Questions? Problems? Need more info? Contact Segment Support for assistance!

Headings:
Redshift cluster and Redshift connector limitations
Reserved words
Table count limitations
Cluster node limitations
Column type changes
VARCHAR size limits
Blocklisted track call properties
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/warehouses/redshift-tuning/
Paragraphs:
On this page
Waiting minutes and minutes, maybe even an hour, for your queries to compute is an unfortunate reality for growing companies. Whether your data has grown faster than your cluster, or you’re running too many jobs in parallel, there are lots of reasons your queries might be slowing down.
To help you improve your query performance, this guide takes you through common issues and how to mitigate them.
As your data volume grows and your team writes more queries, you might be running out of space in your cluster.
To check if you’re getting close to your max, run this query. It will tell you the percentage of storage used in your cluster. Segment recommends that you don’t exceed 75-80% of your storage capacity. If you approach that limit, consider adding more nodes to your cluster.
Learn how to resize your cluster.
Another thing you’ll want to check is if your queries are efficient. For example, if you’re scanning an entire dataset with a query, you’re probably not making the best use of your compute resources.
Some tips for writing performant queries:
Consider usingINNER joinsas they are more efficient thanLEFT joins.
Stay away fromUNIONwhenever possible.
Specify multiple levels of conditionals when you can.
UseEXPLAINto show the query execution plan and cost.
To learn more about writing beautiful SQL, check out these resources:
Periscope on Query Performance
Mode on Performance Tuning SQL Queries
Chartio on Improving Query Performance
Some databases like Redshift have limited computing resources. Running multiple queries or ETL processes that insert data into your warehouse at the same time will compete for compute power.
If you have multiple ETL processes loading into your warehouse at the same time, especially when analysts are also trying to run queries, everything will slow down. Try to schedule them at different times and when your cluster is least active.
If you’re a Segment Business Tier customer, you can schedule your sync times under Warehouses Settings.

You also might want to take advantage of Redshift’sWorkload Managementthat helps ensure fast-running queries won’t get stuck behind long ones.
As mentioned before, Redshift schedules and prioritizes queries usingWorkload Management. Each queue is configured to distribute resources in ways that can optimize for your use-case.
The default configuration is a single queue with only 5 queries running concurrently, but Segment discovered that the default only works well for low-volume warehouses. More often than not, adjusting this configuration can improve your sync times.
Before Segment’s SQL statements, Segment usesset query_group to "segment";to group all the queries together. This allows you to create a queue that isolates Segment’s queries from your own. The maximum concurrency that Redshift supports is 50 acrossallquery groups, and resources like memory distribute evenly across all those queries.
Segment’s initial recommendation is for 2 WLM queues:
a queue for thesegmentquery group with a concurrency of10
leave the default queue with a concurrency of5

Generally, Segment is responsible for most writes in the databases Segment connects to, so having a higher concurrency allows Segment to write as fast as possible. If you’re also using the same database for your own ETL process, you may want to use the same concurrency for both groups. You may even require additional queues if you have other applications writing to the database.
Each cluster may have different needs, so feel free to stray from this recommendation if another configuration works better for your use-case. AWS provides someguidelines, and you can alwayscontact usas Segment is more than happy to share the learnings while working with Redshift.
In addition to following performance best practices, here are a some more optimizations to consider if you’re using Segment Warehouses.
When Segment is actively loading data into your data warehouse, Segment is competing for cluster space and storage with any other jobs you might be running. Here are the parameters that influence your load time for Segment Warehouses.
To make sure you have enough headroom for quick queries while using Segment Warehouses, here are some tips!
Hopefully these steps will help to speed up your workflow! If you need any other help, feel free tocontact us.

Headings:
Speeding Up Redshift Queries
Common Causes for Slow Queries
1. Not enough space
2. Inefficient queries
3. Running multiple ETL processes and queries
4. Default WLM Queue Configuration
Pro-tips for Segment Warehouses
Factors that affect load times
Performance optimizations
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/storage/warehouses/redshift-useful-sql/
Paragraphs:
On this page
Below you’ll find a library of some of the most useful SQL queries customers use in their Redshift warehouses. You can run these in your Redshift instance with little to no modification.
If you’re looking to improve the speed of your queries, check out Segment’sSpeeding Up Redshift Queriespage.
You can use SQL queries for the following tasks:
If you’re looking for SQL queries for warehouses other than Redshift, check out some of Segment’sAnalyzing with SQL guides.
The Track call allows you to record any actions your users perform. A Track call takes three parameters: the userId, the event, and any optional properties.
Here’s a basic Track call:
A completed order Track call might look like this:
Each Track call is stored as a distinct row in a single Redshift table calledtracks. To get a table of your completed orders, you can run the following query:
That SQL query returns a table that looks like this:

But why are there columns in the table that weren’t a part of the Track call, likeevent_id? 
This is because the Track method (for client-side libraries) includes additional properties of the event, likeevent_id,sent_at, anduser_id!
If you want to know how many orders were completed over a span of time, you can use thedate()andcountfunction with thesent_attimestamp:
That query returns a table like this:
To see the number of pants and shirts that were sold on each of those dates, you can query that using case statements:
That query returns a table like this:
Segment’s API does not impose any restrictions on your data with regard to user sessions.
Sessions aren’t fundamental facts about the user experience. They’re stories Segment builds around the data to understand how customers actually use the product in their day-to-day lives. And since Segment’s API is about collecting raw, factual data, there’s no API for collecting sessions. Segment leaves session interpretation to SQL partners, which let you design how you measure sessions based on how customers use your product.
For more on why Segment doesn’t collect session data at the API level,check out a blog post here.
Each of Segment’s SQL partners allow you to define sessions based on your specific business needs. WithLooker, for example, you can take advantage of their persistent derived tables and LookML modeling language to layer sessionization on top of your Segment SQL data. Segment recommendschecking out Looker’s approach here.
To define sessions with raw SQL, a great query and explanation comes fromMode Analytics.
Here’s the query to make it happen, but read Mode Analytics’blog postfor more information. Mode walks you through the reasoning behind the query, what each portion accomplishes, how you can tweak it to suit your needs, and the kinds of further analysis you can add on top of it.
The Identify method ties user attributes to auserId.
As these user traits change over time, you can continue calling the Identify method to update their changes. With this query, you can update Bob’s account plan to “Premium”.
Each Identify call is stored in a single Redshift table calledidentifies. To see how a user’s plan changes over time, you can run the following query:
This SQL query returns a table of Bob’s account information, with each entry representing the state of his account at different time periods:
If you want to see what your users looked like at a previous point in time, you can find that data in theidentifiestable. To get this table for your users, replace ‘initech’ in the SQL query with your source slug.
If you only want the current state of the users, convert theidentifiestable into adistinct users tableby returning the most recent Identify call for each account.
The following query returns theidentifiestable:
That query returns a table like this:
If all you want is a table of distinct user with their current traits and without duplicates, you can do so with the following query:
Let’s say you have anidentifiestable that looks like this:
If you want to query the traits of these users, you first need toconvert the identifies table into a users table. From there, run a query like this to get a count of users with each type of plan:
And there you go: a count of users with each type of plan!
Thegroupmethod ties a user to a group. It also lets you record custom traits about the group, like the industry or number of employees.
Here’s what a basicgroupcall looks like:
As these group traits change over time, you can continue calling the group method to update their changes.
Each group call is stored as a distinct row in a single Redshift table calledgroups. To see how a group changes over time, you can run the following query:
The previous query will return a table of Initech’s group information, with each entry representing the state of the account at different times.
If you want to see a group’s traits at a previous point in time, this query is useful (To get this table for your groups, replace ‘initech’ with your source slug).
If you only want to see the most recent state of the group, you can convert the groups table into a distinct groups table by viewing the most recent groups call for each account.
The following query will return your groups table:
The previous query returns the following table:
However, if all you want is a table of distinct groups and current traits, you can do so with the following query:
This query will return a table with your distinct groups, without duplicates.

Headings:
Useful SQL Queries for Redshift
Tracking events
Grouping events by day
Define sessions
How to define user sessions using SQL
Identify users
Historical traits
Convert the identifies table into a users table
Counts of user traits
Groups to accounts
Historical Traits
Converting the Groups Table into an Organizations Table
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/regional-segment/
Paragraphs:

Headings:
Redirecting…
Crawling: https://segment.com/docs/connections/test-connections/
Paragraphs:
On this page
Segment has an Event Tester that enables you to test your connections between Segment and your destination. You can access the Event Tester from your Source Debugger, or from your destination settings.
Available for server-side event streaming destinations only
This feature is only available for server-side integrations (also known as cloud-mode destinations). You can’t use this for client-side integrations (also known as device-mode destinations).
There are two scenarios where you might want to use the Event Tester:
1. Choose an event from the Source Debugger that you want to debug and select “Validate”
Go to your Source Debugger, select an event and in the top right hand side of the debugger view, select “Validate”.

2. Choose the destination you want to test with
Select the destination that you want to test this event with. At this time, you can only use the Event Tester for cloud-mode (server side) destinations.

3. Send event to destination
The event payload from your debugger that you just selected will automatically load in the JSON view. You have the option to edit the payload if you want. Assuming it looks good, select “Send Event” at the bottom right of the screen.

4. Ensure you’re happy to send the test event to the destination
This is a real event that will appear in your end tool alongside your existing data. If you’re not comfortable with this, then select “Cancel” and do not send the event.

5. View the Partner API response
On the right hand side of the Event Tester you will see the response from the partner API. At the top, Segment provide of summary of the response. Below is the raw response payload Segment received that you can use for further debugging if necessary.

If you are receiving an error and are unsure how to fix the issue, visit the partner docs (for examplehttps://developers.google.com/analytics/devguides/reporting/core/v3/errors) or contact the partner support team.
The Event Tester is only accessible to users with write access in their Segment workspace (read-only users will not see the Event Tester in their workspace).
If you experience an error,let Segment knowand the Segment team will help you troubleshoot the issue.
The Event Tester is not available for Data Lakes.
Events passed into the Event Tester bypass destination filters. Destination filters are applied to events as they are sent to specific destinations. However, the Event Tester is designed to help you troubleshoot your Sources, their configuration, and their downstream destinations by showing a sample of the data available. It allows you to check that data is being sent, and that it’s in the correct format without the filters being applied. This means that when you use the Event Tester, you’re seeing the data before any destination filters or other processing rules are applied, providing a clear view of the raw event data as it comes from the source.
Headings:
Event Tester
Use Cases
Ensuring an event is successfully making it to a specific destination
FAQ
Why can’t I see the Event Tester when I log into my workspace?
The Event Tester experienced an error when sending my event. Why did this happen?
Is this feature available for Data Lakes?
Why are my destination filters being ignored?
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/data-export-options/
Paragraphs:
There are a few ways to export your Segment data. SegmentBusiness customershave the most data flexibility, but our self-service customers also have options.
Customers on ourbusiness plancan take advantage of Replay when they change vendors or add a vendor to their marketing and analytics stack.
When you want to trial or start using a new vendor, Segment canreplayyour timestamped, historical data so it’s like you’ve been using that app all along.
Replay works for all server-side destinations that have or accept timestamps, including our Amazon S3 destination, so you can get all your data history since the first event you sent to Segment.
If you are on any of our plans, there are multiple options available to you to gain access to your raw data.
All customers can connect adata warehouseto Segment – Free and Team customers can connect one, while Business customers can connect as many as they need. We translate and load your raw data logs into your warehouse for more powerful analysis in SQL.
We store all your API calls as line-separated JSON objects in Amazon S3. If youenable Amazon S3in your destinations catalog, we will copy the same data to your own S3 bucket. The data copied will only include data sent to Segment after you turn on the destination. Read ourAmazon S3 docsto learn more about how we structure that data.
You can use ourwebhooks destinationto fire off requests in realtime to an endpoint that you would need to spin up and manage on your side. This is basically re-creating how our business system works but takes a bit of work on your side. If your event volume is high it can be difficult to keep a server up to receive those messages in realtime.
Another one of our destinations isIron.io. They function similar to webhooks, but they will manage the message queue and allow you to run scripts on your data before routing it to another end point. Again this is similar to what Segment does for our business customers, but will require a decent amount of work from your team, however it will be much more reliable if your event volume gets high.
This option is the most restrictive but might be the easiest if you need only basic data to be exported. A few examples would be to use the reporting APIsClickyorGoogle Analyticsprovide (after turning those tools on in Segment and sending them data). Those APIs aren’t super flexible and you won’t see all the data from Segment, but for basic metrics they should work. One tool that’s a bit more flexible when it comes to a reporting API isKeen.io, which is also available on the Segment platform.
This page was last modified: 01 Dec 2022
Questions? Problems? Need more info? Contact Segment Support for assistance!
Thanks for your feedback!
Can we improve this doc?Send us feedback!


Headings:
Data Export Options
Business plan customers
Replay
Free, Team, and Business plan customers
Warehouses
S3 Logs
Webhooks
Iron.io
3rd Party Reporting APIs
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/destination-data-control/
Paragraphs:

Headings:
Redirecting…
Crawling: https://segment.com/docs/connections/event-delivery/
Paragraphs:
On this page
The Event Delivery tool helps you understand if data is reaching your destinations, and also helps you to see if Segment encountered any issues delivering your source data.
Segment sends billions of events to destinations every week. If Segment encounters any errors when attempting to deliver your data, Segment reports them in the Event Delivery tool.
Available for server side event streaming destinations only
This feature is only available for server side integrations (also known as cloud-mode destinations). You will not be able to use this for client side integrations (also known as device-mode destinations) because device-mode data is sent directly to the destination tool’s API. In order to report on deliverability, the data must be sent to destinations using a server side connection.
Not available for Warehouses or Amazon S3. These destinations work differently from other destinations in Segment, and aren’t supported at this time.
Here’s an example of what the Event Delivery tool looks like:

Scenarios when this tool will be useful:
Event Delivery can be accessed within any supported destination in the App. It’s located on the tab under “Settings” for each destination.

The UI consists of three key parts that report on Segment’s ability to deliver your source data - Key Metrics, Error Details, and Delivery Trends. Reporting on core functionality from top to bottom:
There’s a drop down menu to select your time period. All of the UI is updated based on the time period you select.

From left to right in the above graphic:
Delivered:This tells you the number of messages Segment successfully delivered to a destination in the time period you selected.
Not Delivered:This count tells you the number of messages Segment was unable to deliver. If this number is higher than zero, you will see the reasons for this failure in the errors table below.
P95 Latency:This is the time it takes for Segment to deliver the slowest 5% of your data (known as P95 latency). The latency reported is end-to-end: the event being received through the Segment API to the event being delivered to partner API. This helps tell you if there is a delay in your data and how bad it is.
The purpose of the table is to provide you a summary of the different errors we’ve seen in a given period and the most important information on them. All of the rows in the table are clickable and expand to give you more information.

View Segment’s list ofIntegration Error Codesfor more information about what might cause an error.
When there’s an error, Segment wants to give you as much information as possible to help you resolve the issue. See below for an example of what this view looks like.

This view includes:
The event delivery UI provides a human-friendly summary of the error, based on the payload Segment received back from the partner.
These are actions you can take, based on what Segment knows about the issue.
This section displays links to any documentation that might be helpful to you.
To help you debug, Segment provides sample payloads from every step of the data’s journey:
You Sent- the data you sent to Segment’s API.
Request to Destination- the request Segment made to the Partner API. This payload will likely be different from what you sent it because Segment is mapping your event to the partner’s spec to ensure the message is successfully delivered.
Response from Destination- the response Segment received from the Partner API. This will have the raw partner error. If you need to troubleshoot an issue with a Partner’s Success team, this is usually something they’ll want to see.
You can opt in to receive email alerts regarding failed events/syncs by going to your workspaace Settings > User Preferences > Activity Notifications > Destiantions. Then you can select the option to be alerted by Email and/or In-app.
When debugging, it’s helpful to see when issues start, stop, and trend over time.

Delivered: The number of events that were successfully delivered in the time period you selected.
Not delivered: The number of events that were not successfully delivered in the time period you selected.
How P95 latency has trended over the time period you selected.
This page was last modified: 05 Mar 2024
Questions? Problems? Need more info? Contact Segment Support for assistance!
Headings:
Event Delivery
When to use Event Delivery
Where do I find it?
How do I use Event Delivery?
1. Time period
2. Key Metrics
3. Error details
Error detail view
Description
Actions
More Info
Sample payloads
Email Alerts
4. Trends
Event Delivery
Latency
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/delivery-overview/
Paragraphs:
On this page
Delivery Overview is a visual observability tool designed to help Segment users diagnose event delivery issues for any cloud-streaming destination receiving events from cloud-streaming sources.
Delivery Overview for RETL destinations and Engage Audience Syncs currently in development
This means that Segment is actively developing Delivery Overview features for RETL destinations and Engage Audience syncs. Some functionality may change before Delivery Overview for these integrations becomes generally available.
Delivery Overview is generally available for streaming connections (cloud-streaming sources and cloud-streaming destinations) and in public beta for storage destinations. Some metrics specific to storage destinations, like selective syncs, failed row counts, and total rows seen, are not yet available. 
All users of Delivery Overview have access to the Event Delivery tab, and can configure delivery alerts for their destinations.
Delivery Overview has three core features:
You can refine these tables using the time picker and the metric toggle, located under the destination header. With the time picker, you can specify a time period (last 10 minutes, 1 hour, 24 hours, 7 days, 2 weeks, or a custom date range over the last two weeks) for which you’d like to see data. With the metric toggle, you can switch between seeing metrics represented as percentages (for example,85% of eventsora 133% increase in events) or as counts (13 eventsoran increase of 145 events.) Delivery Overview shows percentages by default.
The pipeline view provides insights into each step your data is processed by enroute to the destination, with an emphasis on the steps where data can be discarded due to errors or your filter preferences. Each step provides details into counts, change rates, and event details (like the associated Event Type or Event Names), and the discard steps (Failed on ingest, Filtered at source, Filtered at destination, & Failed delivery) provide you with the reasons events were dropped before reaching the destination. Discard steps also include how to control or alter that outcome, when possible. The pipeline view also includes a label between the Filtered at destination and Failed delivery steps indicating how many events are currently pending retry.
Lookback window
Delivery Overview applies a 5-minute lookback period to provide stable, accurate metrics across all pipeline steps. This interval accounts for processing delays and ensures the data Segment displays reflects a reliable snapshot of recent events.
The pipeline view for classic destinations includes the following steps:
The pipeline view for Actions destination includes the following steps:

The pipeline view for storage destination includes the following steps:
The following image shows a storage destination with 23 partially successful syncs:

The breakdown table provides you with greater detail about the selected events.
To open the breakdown table, select either the first step in the pipeline view, the last step in the pipeline view, or select a discard step and then click on a discard reason.
The breakdown table displays the following details:
1:Segment calculates the related change percentage by subtracting the percent of events impacted in the previous time period from the percent of impacted events in the current time period. For example, if last week 15% of your events were filtered at a source, but this week, 22% of your events were filtered at a source, you would have a related change percentage of 7%.
The discard table provides you with greater detail about the events that failed to deliver or were filtered out of your sources and destinations.
To open the discard table, click on one of the discard steps. If you click on a row in the discard table, you can see the breakdown table for the discarded events.
The discard table displays the following details:
1:Segment calculates the related change percentage by subtracting the percent of events impacted in the previous time period from the percent of impacted events in the current time period. For example, if last week 15% of your events were filtered at a source, but this week, 22% of your events were filtered at a source, you would have a related change percentage of 7%.
Delivery Overview is useful to diagnose delivery errors in the following scenarios:
Delivery Overview in Engage Destinations
Because Engage uses sources for multiple purposes, you can expect to seefiltered at destinationevents with the integrations object in destinations linked to Engage. Engage uses the integrations object to route events to destinations you’ve added to your audiences, traits, and journey steps. As a result, some events aren’t meant to be delivered by the destination, so the integrations object filters them.
To view the Delivery Overview page:
To use Delivery Overview:
With Source Debugger or Event Delivery, you can only verify that events are successfully making it from your source or to your destination. If events fail, you have to troubleshoot to see where in the pipeline your events are getting stuck. With Event Tester, you can verify that your event makes it from your source to your destination, but if the results aren’t what you expected, you’re stuck troubleshooting your source, filters, tracking plans, and destinations.
With Delivery Overview, you can verify that your source receives your events, that any filters and tracking plans work as expected, and that events successfully make it to your destination. Any errors or unexpected behavior can be identified using the pipeline view, leading to quicker resolution.
You can use the Event Delivery alerting features (Delivery Alerts) by selecting theAlertstab in the destination header. Once you enable alerts, if the successful delivery rate of all events is less than the threshold percentage in the last 24 hours, you’ll be notified through in-app notification and/or workspace email.
Note that this is dependent on yournotification settings. For example, if the threshold is set to 99%, then you’ll be notified each time less than 100% of events fail.
You can also useConnections Alerting, a feature that allows Segment users to receive in-app, email, and Slack notifications related to the performance and throughput of an event-streaming connection.
Connections Alerting allows you to create two different alerts:
The data in Delivery Overview has an expected latency of approximately 30 seconds after event ingestion, but this may vary, depending on the features you’ve enabled in your workspace and spikes in volume. Segment delays the data visible in the Delivery Overview UI by 5 minutes to allow for more precise metric correlation. Segment does not impose the 5 minute delay if you access data using the Public API.
Similar to Segment’sEvent Deliveryfeature, the Delivery Overview page is only available for server-side integrations (also known as cloud-mode destinations). You won’t be able to use the Delivery Overview page for client side integrations (also known as device-mode destinations) because device-mode data is sent directly to the destination tool’s API. In order to report on deliverability, data must be sent to destinations using a server-side connection.
The Delivery Overview pipeline steps Failed on Ingest, Filtered at Source, Filtered at Destination, and Failed Delivery display adiscard tablewith information about why your events failed or were discarded.
This table provides a list of all possible discard reasons available at each pipeline step.
Show allAllFailed on IngestFiltered at SourceFiltered at DestinationFailed Delivery

Headings:
Delivery Overview
Key features
Pipeline view
Classic destinations
Actions destinations
Storage destinations
Breakdown table
Discard table
When should I use Delivery Overview?
Where do I find Delivery Overview?
How do I use Delivery Overview?
How does Delivery Overview differ from other Segment monitoring and observability tools?
How can I configure alerts?
How “fresh” is the data in Delivery Overview?
Why is the Delivery Overview page only available for cloud-mode destinations?
Troubleshooting
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/alerting/
Paragraphs:
On this page
Connections Alerting allows Segment users to receive in-app, email, and Slack notifications related to the performance and throughput of an event-streaming connection.
To access Connections Alerting, select an event-streaming connection (like a web library source or cloud mode destination) and click theAlertstab.
On the Alerts tab, you can create alerts and view all active alerts for this connection. You can only edit or delete the alerts that you create.
You can create an alert that notifies you when the volume of events received by your source in the last 24 hours changes beyond a percentage you set. For example, if you set a change percentage of 4% and your source received 100 events over the first 24 hours, Segment would notify you the following day if your source ingested fewer than 96 or more than 104 events.
To receive a source volume alert in a Slack channel, you must first create a Slack webhook. For more information about Slack webhooks, see theSending messages using incoming webhooksdocumentation.

To create a source volume alert:
To make changes to a source volume alert, select the icon in the Actions column for the alert and clickEdit.
To delete a source volume alert, select the icon in the Actions column for the alert and clickDelete.
Deleting alerts created by other users requires Workspace Owner permissions
All users can delete source volume alerts that they created, but only those with Workspace Owner permissions can delete alerts created by other users.
You can create an alert that notifies you when the volume of events successfully received by your destination in the last 24 hours falls below a percentage you set. For example, if you set a percentage of 99%, Segment notifies you if your destination had a successful delivery rate of 98% or below.
To receive a successful delivery rate alert in a Slack channel, you must first create a Slack webhook. For more information about Slack webhooks, see theSending messages using incoming webhooksdocumentation.
To create a successful delivery rate alert:
To make changes to a successful delivery rate alert, select the icon in the Actions column for the alert and clickEdit.
To delete a successful delivery rate alert, select the icon in the Actions column for the alert and clickDelete.
Deleting alerts created by other users requires Workspace Owner permissions
All users can delete successful delivery alerts that they created, but only those with Workspace Owner permissions can delete alerts created by other users.
Segment generates delivery alerts for failed deliveries and successful deliveries, which are the last two stages of the delivery pipeline. As a result, alerts are based on Segment’s attempts to send qualified events to your destination, excluding those filtered out by business rules (like protocols, destination filters, or mappings).
This page was last modified: 02 Jul 2024
Questions? Problems? Need more info? Contact Segment Support for assistance!
Thanks for your feedback!
Can we improve this doc?Send us feedback!
On this page
Was this page helpful?
Thanks for your feedback!
Can we improvethis doc?Send us feedback!
Product
For Developers
Company
Support
© 2025 Segment.io, Inc.

Headings:
Connections Alerting
Source volume alerts
Successful delivery rate alerts
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/find-writekey/
Paragraphs:
On this page
The write key is a unique identifier for each source. It lets Segment know which source is sending the data and which destinations should receive that data.
To find a write key, you first need to create anevent streams sourcelike a website, server, or mobile source. (Cloud-sourcesdo not have write keys, as they use a token or key from your account with that service.)
Then, in the Source, go toSettingsand selectAPI Keys.

Now you can add the source’s write key to your app and begin sending data to Segment.
To find the source given a write key within your workspace, open your workspace and select the search icon. Enter your write key into the search bar. If the write key exists in the workspace and is connected to a source, the source shows up in the list of results.

This method is only available to locate event streams sources
This method cannot be used to find a destination or cloud event source.
This page was last modified: 20 Nov 2023

Headings:
Locate your Write Key
Find the write key for a source
Locate a source using your write key
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/integration_error_codes/
Paragraphs:
This page was last modified: 06 Jul 2022
Questions? Problems? Need more info? Contact Segment Support for assistance!
Thanks for your feedback!
Can we improve this doc?Send us feedback!
Was this page helpful?
Thanks for your feedback!
Can we improvethis doc?Send us feedback!
Product
For Developers
Company
Support
© 2025 Segment.io, Inc.

Headings:
Integration Error Codes
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/rate-limits/
Paragraphs:
On this page
These limits were updated on January 25, 2024.
Events ingested by Segment have a limit of10,000properties per individual event received. For example, two Track events named “Page Viewed” and “Signup completed” each have their own limit. Segment will not persist properties beyond this limit, and will drop any corresponding values.
If any sources send more than 1,000 events per second in a workspace without prior arrangement, Segment reserves the right to queue any additional events and process those at a rate that doesn’t exceed this limit. To request a higher limit, contactSegment.
Engage rate limit
Engage has a limit of 1,000 events per second for inbound data. Visit theEngage Default Limits documentationto learn more.
Most destinations have their own rate limits that Segment cannot control. In some instances, Segment is able to ingest and attempt to deliver data faster than the downstream destination is able to accept data. Outbound requests to a destination may also fail for other reasons outside of Segment’s control. When requests to downstream destinations fail, Segment makes additional attempts to deliver the data (retries). However, when more than 1,000 requests per second to a downstream destination fail or when the failure rate for a downstream destination exceeds 50% for more than 72 hours, Segment reserves the right to reduce the number of retries until the condition is resolved.

Headings:
Product Limits
Event properties ingestion limit
Inbound data ingestion API rate limit
Outbound downstream destination rate limits
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/oauth/
Paragraphs:
OAuth 2.0 is available to customers on Business tier plans.See theavailable plans, orcontact Support.
On this page
OAuth 2.0 is an online authorization standard that uses tokens to grant access to API resources like Segment’s tracking API. You can use OAuth 2.0 as a security requirement for connections to third-party tools.
Depending on your workspace permissions, your access to OAuth apps is limited.
You must have already created a workspace in Segment to use OAuth.
To create a new OAuth application:
Enter the configuration settings:
Once you create your OAuth app, you can now connect a source to your OAuth app.
OAuth only supports server-side sources. See the list ofsupported sources.
To connect a source to OAuth:
To disconnect your source from OAuth, clickDisconnect.
Once you’ve connected your source to OAuth, you can enable it. To enable your source:
To disable your source from OAuth, turn the toggle off forEnable OAuth.
You can obtain an access token once you create an OAuth application and enable a source to OAuth.
Access tokens are only valid within a region. The supported regional authorization servers are:
To obtain the access token:
Create a JWT token with the header and payload as below:
Header
Payload
Send a form-url-encodedPOSTrequest to the regional authorization server’s\tokenroute with the following parameters:
To use the access token, see an example of how to use the access token in theHTTP API source.
To edit an existing OAuth application:
To delete an OAuth app, you must remove all connected sources from the app.
To delete an OAuth app:
When security incidents expose access tokens, you can revoke your access token. To revoke a token:
OAuth 2.0 currently supports these sources:
OAuth 2.0 currently supports these scopes:
Tracking API scopes
Source Functions scopes
Public API scopes

Headings:
OAuth 2.0
Permissions
Create an OAuth app
Connect a source to OAuth
Enable a source to OAuth
Obtain the access token
Edit an OAuth application
Delete an OAuth app
Revoke a token
Supported sources
Supported scopes
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/connections/aws-privatelink/
Paragraphs:
On this page
Amazon Web Services’ PrivateLinkis an AWS service that provides private connectivity between VPCs without exposing traffic to the public Internet. Keeping traffic in the Amazon network reduces the data security risk associated with exposing your Warehouse traffic to the Internet.
Segment’s PrivateLink integration is currently in private beta and is governed by Segment’sFirst Access and Beta Preview Terms. You might incur additional networking costs while using AWS PrivateLink.
You can configure AWS PrivateLink forDatabricks,RDS Postgres,Redshift, andSnowflake. Only warehouses located in regionsus-east-1,us-west-2, oreu-west-1are eligible.
Usage limits for each customer during the AWS PrivateLink Private Beta include the following:
The following Databricks integrations support PrivateLink:
Segment recommends reviewing the Databricks documentation before attempting AWS PrivateLink setup
The setup required to configure the Databricks PrivateLink integration requires front-end and back-end PrivateLink configuration. Review theDatabricks documentation on AWS PrivateLinkto ensure you have everything required to set up this configuration before continuing.
Before you can implement AWS PrivateLink for Databricks, complete the following prerequisites in your Databricks workspace:
To implement Segment’s PrivateLink integration for Databricks:
The following RDS Postgres integrations support PrivateLink:
Before you can implement AWS PrivateLink for RDS Postgres, complete the following prerequisites:
To implement Segment’s PrivateLink integration for RDS Postgres:
The following Redshift integrations support PrivateLink:
Before you can implement AWS PrivateLink for Redshift, complete the following prerequisites:
To implement Segment’s PrivateLink integration for Redshift:
The following Snowflake integrations support PrivateLink:
Before you can implement AWS PrivateLink for Snowflake, complete the following prerequisites:
To implement Segment’s PrivateLink integration for Snowflake:

Headings:
Amazon Web Services PrivateLink
Databricks
Prerequisites
Implement PrivateLink for Databricks
RDS Postgres
Prerequisites
Implement PrivateLink for RDS Postgres
Redshift
Prerequisites
Implement PrivateLink for Redshift
Snowflake
Prerequisites
Implement PrivateLink for Snowflake
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/unify/
Paragraphs:
Unify requires a Business tier account and is included with Engage.See theavailable plans, orcontact Support.
On this page
Use Segment Unify, formerly known as Profiles, for a complete view of your customers.
WithIdentity Resolution, track every interaction across the entire user journey to create unified, real-time customer identities. View user profiles in one place through theProfile explorerin the Segment app. Use theProfile APIto programmatically query user profiles, traits, and events.
You can then use this interaction data with customer engagement tools, such as Engage, to deliver personalized, omnichannel experiences.
If you need to troubleshoot or learn about your profile data, useProfiles Insightsfor a transparent view of your Unify profiles.
Unify is an add-on to Segment Connections Business Tier. It’s also a required add-on for Twilio Engage.
To useComputed TraitsandAudienceswith Unify, you must have access to Engage.
To set up and get data flowing through Unify, visit Segment’sOnboarding Guide.
SetIdentity Resolutionrules to take event data from across devices and channels and intelligently merge it into complete user- or account-level profiles. This enables you to understand customer behavior as it evolves in real-time across multiple touchpoints.
With Identity Resolution:
Visit Segment’sIdentity Resolution docsto learn more.
Use the Profile explorer to view all user data, including their event history, traits, and identifiers.
With the Profile explorer, you have a complete view of your customers.
If you’re using Engage, use the Profile explorer to view audiences, traits, journey membership, andsubscription statesfor email and phone numbers.
With Unify Plus, you can add detail to user profiles with new traits and use them to power personalized marketing campaigns. Add new traits to your user or account profiles using:
Use Segment’s Profile API to programmatically access all traits stored for a user. This includes theexternal_ids,traits, andeventsthat make up a customer’s journey with your product.
Use the Profile API to help your organization:
Visit Segment’sProfile API docfor more information.
Use Profiles Insights to troubleshoot your event data with a transparent view of your Unify profiles.
Learn about your events and identifiers on your profiles and answer questions such as why two profiles didn’t merge, why an event wasn’t resolved to a profile, or why an external ID isn’t present.
Visit theProfiles Insightsdoc to learn more.
Use Profiles Sync to connect identity-resolved customer profiles to a data warehouse of your choice.
With a continual flow of synced profiles, teams can enrich and use these data sets as the basis for new audiences and models. Profiles Sync addresses a number of use cases, with applications for identity graph monitoring, attribution analysis, machine learning, and more.
Visit theProfiles Sync Setupdoc to learn more.
For Engage users, after you set up your identity rules and have data flowing through Unify, you can activate profiles to deliver personalized engagement experiences. Visit theEngage docsto learn more.

Headings:
Unify Overview
Getting started
Identity Resolution
Profile explorer
Enrich profiles with traits
Profile API
Profiles Insights
Profiles Sync
Next steps: activate your profiles with Engage
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/unify/quickstart/
Paragraphs:
Unify requires a Business tier account and is included with Engage.See theavailable plans, orcontact Support.
On this page
This guide walks you through the set up process for a simple Unify space, which you can use if your Segment implementation is simple. If your implementation is complex, you can use this to demonstrate and test Unify before working on a more complex configuration.
If you’re using Engage, visit theEngage Foundations Onboarding Guidefor additional steps to create audiences, connect to destinations, and more.
To configure and use Unify, you need the following:
When you first start working with Unify, you should start by creating a “Developer” space. This is your experimental and test environment while you learn more about how Unify works. You can validate that identity resolution is working correctly in the Developer space, and then apply those changes to yourProductionspace once you’re sure everything is working as expected.
This two-space method prevents you from making untested configuration changes that immediately affect production data.
You probably have teammates who help set up your Segment Workspace with the data you need. Invite them to your Unify dev space and grant them access to the space. Navigate toAccess Managementin your workspace settings to add them.
If the source you want to add doesn’t appear on the list, then check if the source is enabled. If the source is enabled, verify that you have set up a connection policy which enforces that you can only add sources with specific labels to this space. Read more about Segment’s connection policy in theSpace Setupdocs.
Tip:It sounds a little counter- intuitive to connect a production source to a developer space, but your production sources have rich user data in them, which is what you need to build and validate user profiles.
Once you select sources, Segment starts a replay of one month of historical data from these sources into your Unify space. Segment does this step first so you have some user data to build your first profiles.
The replay usually takes several hours, but the duration will vary depending on how much data you have sent through these sources in the past one month. When the replay finishes, you are notified in the Sources tab under Settings, shown below.
Note: Data replays start with the earliest (oldest) chronological events in the one month window, and finish with the most recent. Don’t continue to the next step until all replays are marked complete. If you do, the data in your Unify data will be stale.
Once the Source(s) finish replaying, data from your connected Sources flows into Unify in near real time, just like it does for sources in your Segment workspace.
Once the replay finishes, you can see the data replayed into Unify using the Profile explorer. You should have a lot! The data should include information from multiple sources and multiple sessions, all resolved into a single profile per user.
Before you continue, check a few user profiles to make sure they show an accurate and recent snapshot of your users.
A good test is to look atyour ownuser profile, and maybe some colleagues’ profiles. Look in the Profile explorer for your Profile, and look at your event history, custom traits and identifiers. If these identifiers look correct across a few different profiles (and you can verify that they are all correct), then you’re ready to create an audience.
If your user profiles look wrong, or you aren’t confident users are being accurately defined and merged, stop here and troubleshoot. It’s important to have accurate identity resolution before you continue. See thedetailed Identity Resolution documentationto better understand how it works, and why you may be running into problems. (Still need help?Contact Segmentfor assistance.)
Once you validate that your data is flowing through Unify, you’re ready to create a Production space. Segment recommends that you repeat the same steps outlined above, focusing on your production use cases and data sources.
If you’re using Engage, view additional steps to complete your space set up in theEngage Foundations Onboarding Guide.
You can rename the Segment space UI name, but can’t modify the space slug. As a result, you can’t change the URL of a space.

Headings:
Unify Onboarding Guide
Unify configuration requirements
Step 1: Create a new Developer space
Step 2: Invite teammates to your Segment space
Step 3: Connect production sources
Step 4: Check your profile data
Step 5: Create your production space
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/unify/identity-resolution/
Paragraphs:
Unify requires a Business tier account and is included with Engage.See theavailable plans, orcontact Support.
On this page
Identity Resolution sits at the core of Segment. The Identity Graph merges the complete history of each customer into a single profile, no matter where they interact with your business. Identity Resolution allows you to understand a user’s interaction across web, mobile, server, and third-party partner touch-points in real time, using an online and offline ID graph with support for cookie IDs, device IDs, emails, and custom external IDs. If you are sending theGroup call, you can also understand user behavior at the account-level.

For security reasons, Segment requires that theProfile APIonly be used server-side. The Profile API allows you to look up data about any user given an identifier (for example, email,anonymousId, oruserId) and an authorized access secret. While this enables powerful personalization workflows, it could also let your customers’ data fall into the wrong hands if the access secret were exposed on the client.
Instead, by creating an authenticated personalization endpoint server-side backed by the Profile API, you can serve up personalized data to your users without the risk of their information falling into the wrong hands.

Headings:
Identity Resolution Overview
Identity Graph
Highlights
Technical highlights
FAQs
Can I use the Profile API on the client-side?
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/unify/identity-resolution/identity-resolution-onboarding/
Paragraphs:
Unify requires a Business tier account and is included with Engage.See theavailable plans, orcontact Support.
On this page
The steps in this guide pertain to spaces created afterOctober 5th, 2020. For spaces created beforeOctober 5th, 2020, please refer toIdentity Resolution Settings.
Workspace owners, administrators, and users with the Identity Admin role can edit Identity Resolution Settings.
Segment creates and merges user profiles based on a space’s Identity Resolution configuration. Segment searches for identifiers such asuserId,anonymousId, andemailon incoming events and matches them to existing profiles or creates new profiles. These identifiers display in the Identities tab of a User Profile in the Profile explorer.
Navigate toUnify > Profile explorerto view identities attached to a profile, along with custom traits, event history, and more.

After receiving a new event, Segment looks for profiles that match any of the identifiers on the event.
Based on the existence of a match, one of three actions can occur:
1: Create a new profileWhen there are no pre-existing profiles that have matching identifiers to the event, Segment creates a new user profile.
2: Add to existing profileWhen there is one profile that matches all identifiers in an event, Segment attempts to map the traits, identifiers, and events on the call to that existing profile. If there is an excess of any identifier on the final profile, Segment defers to the Identity Resolution rules outlined below.
3: Merge existing profilesWhen there are multiple profiles that match the identifiers in an event, Segment checks the Identity Resolution rules outlined below, and attempts to merge profiles.
Identity Admins should first configure Identity Resolution Settings to protect the identity graph from inaccurate merges and user profiles.
During the space creation process, the first step is to choose an Identity Resolution configuration. If this is your first space, you have the option to choose a Segment-suggested Out-of-the-Box configuration or a custom Identity Resolution setup. All other spaces have a third option of importing settings from a different space.

For most first-time users, Segment recommends that you use the out-of-the-box configuration and answer a short series of questions for a best-fit setup for your use-case.
If you have custom unique identifiers or don’t have a canonicaluser_id, you’re automatically redirected to the Identity Resolution Settings page to complete your setup.
If you’re familiar with identity or have custom identifiers, Segment recommends that you select Custom Rules.
Segment redirects you to the Identity Resolution Settings page where you can add Default Identifiers or Custom Identifiers.
Segment’s 11 default are:
You can also provide a trait or property key to match on to add custom identifiers. You can preview the locations where Segment looks for the identifier. Segment accepts both camelCase and snake_case for context.traits, traits, and properties, but accepts lowercase types for identifiers only in the context.externalIds object.

Segment recommends that you proactively prevent using certain values as identifiers. While these values remain in the payload on the event itself, it is not promoted to an identifier Segment uses to determine user profiles.
This is important when developers have a hard-coded value for fields likeuser_idduring QA or development that then erroneously make it to production. This may cause hundreds of profiles to merge incorrectly and can have costly consequences if these spaces already feed data into a production email marketing tool or push notification tool downstream.
In the past, Segment has seen certain default values that cause large amounts of profiles to merge incorrectly. Segment suggests that for every identifier, customers opt into automatically blocking the following suggested values:

Before sending data through, Segment also recommends that you add any default hard-coded values that your team uses during the development process, such asvoidorabc123.
Identity Admins can specify the total number of values allowed per identifier type on a profile during a certain period. For example, in the image below, theanonymous_idfield has a limit of5 Weekly.
This will vary depending on how companies define a user today. In most cases, companies rely onuser_idto distinguish user profiles and Segment defaults to the following configurations:
Specific cases may deviate from this default. For example, a case where a user can have more than oneuser_idbut one email, like whenshopify_idand an internal UUID define a user. In this case, an example configuration may be:
When you choose the limit on an identifier, ask the following questions about each of the identifiers you send to Segment:
Segment considers the priority of an identifier once that identifier exceeds the limit on the final profile.
For example, consider a Segment space with the following Identity Resolution configurations:
A profile already exists withuser_idabc123andemailjane@example1.com. A new event comes in with newuser_idabc456but the sameemailjane@example1.com.
If this event maps to this profile, the resulting profile would then contain twouser_idvalues and oneemail. Given thatuser_idhas a limit of 1, this exceeds the limit of that identifier. As a result, Segment checks the priority of theuser_ididentifier. Becauseemailanduser_idare the two identifiers on the event andemailranks lower thanuser_id, Segment demotesemailas an identifier on the incoming event and tries again.
At this point, the event searches for any profiles that match just the identifier user_idabc456. Now there are no existing profiles with this identifier, so Segment creates a new profile with user_idabc456.
By default, Segment explicitly orders user_id and email as rank1and2, respectively. All other identifiers are in alphabetical order beginning from rank3. This means that if the identifiers sent with events flowing into Segment are user_id, email, anonymous_id, and ga_client_id, the rank would be as follows:
If a new android.id identifier appeared without first giving it explicit order, the order would automatically reshuffle to:
If you require an explicit order for all identifiers, configure this in the Identity Resolution Settings page before sending in events.

When choosing the priority of your identifier, ask the following questions about each of the identifiers you send to Segment:
This option is available to new spaces after you create an initial Dev space. Segment recommends this option when identity settings are validated as correct in the initial Dev space and should be copied into the Production space.
You can review the identifiers, priorities, limits, and blocked values before you complete the import.

After you configure Identity Resolution settings, the next step is to connect asourceto the Segment space.
After you connect a source, Segment creates user profiles based off of replayed and newly incoming data.

The next step, which is important in the Dev space, is to create an audience to ensure that user profiles have populated correctly and that the Identity Resolution settings follow expected business logic.
For example, if there should be 100,000 distinct users who have auser_id, this would be a great way to validate that the Identity Resolution settings have calculated profiles correctly.
For more information about how to create audiences and traits, see Segment’sAudiences docs.

Headings:
Identity Resolution Onboarding
Flat matching logic
Identity Resolution settings
Out-of-the-box
Custom rules
Blocked values
Limit
Priority
Importing from an existing space
Connect a source
Create an audience
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/unify/identity-resolution/space-setup/
Paragraphs:
Unify requires a Business tier account and is included with Engage.See theavailable plans, orcontact Support.
On this page
When starting with Unify, begin by creating aDevspace. This will be your sandbox instance of Unify to test new Identity settings, audiences, and traits before applying the same changes to aProdspace that would immediately affect production data flowing to downstream destinations.
Before you connect any source to the Dev space, Segment recommends that you first start by reviewing and configuring your Identity settings, as changes to the Identity rules will only be applied to new events received following any updates. Read more on those settings in theIdentity Resolution Settingsdocs.
If you haven’t already, Segment highly recommends labeling all your sources withDevorProdenvironments. Once your sources have been labeled, visit theConnection Policypage by navigating toUnify > Unify settings > Space management. Here, you can enforce that only sources labeledDevcan be connected to yourDevUnify instance.

Note:The Identity Resolution table can only be edited by workspace owners and users with the Identity Admin role.
Once your connection policy is in place, select theProfile sourcestab inUnify settings. Now you can connect a few sources that will automatically begin to replay.
Once the sources have finished replaying, check user profiles to ensure that profiles are merging as expected. This would also be an ideal time to create test audiences and confirm that these populate the expected number of users.
Connect test audiences or traits to a dev instance of your downstream destination. Confirm that users are appearing as expected.
Once everything looks good to go, create a newProdspace, following all the same steps above, and connect a live instance of your downstream destination to yourProdspace.

Headings:
Space Setup
Step one: Create a new Dev space
Step two: Configure Identity settings
Step three: Set up a connection policy
Step four: Connect sources and create test audiences
Step five: Connect audiences to a Dev instance of a downstream destination
Step six: Apply changes to Prod sources
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/unify/identity-resolution/use-cases/
Paragraphs:
Unify requires a Business tier account and is included with Engage.See theavailable plans, orcontact Support.
On this page
Identity Resolution helps create a unified view of the user across devices, apps, and unique identifiers. Identity resolution is critical to understanding the customer journey at multiple touch points, which allows brands to deliver personalized experiences to its customers at scale.
Identity Resolution allows a company to link a customer’s journey from pre-account creation to post-account activity. This is important to help a brand understand the behaviors that lead a user to convert from a window shopper in the discovery stage to a buyer with intent in the consideration and decision stage to the loyal return customer in the conversion and retention stage.
By linking any anonymous events a user had before creating an account to a user’s logged-in activity, a marketing team can now have a complete understanding of a user’s past interactions with your app.
This can lead to invaluable insights into the behaviors and triggers in an app that motivate a user to register for an account.
Users can have multiple touch points with an app ecosystem through more than one device. For example, users might view an eCommerce site through a mobile native app, a mobile web browser, or a desktop web browser.
By tracking a user’s activity across all platforms, brands will be able to more efficiently target campaigns to users as they’ll have the knowledge of funnels that complete across devices.
For example, a user who adds a product to a cart on the iPhone app but completes the checkout on the Android app shouldn’t be targeted with abandoned cart push notifications on the iPhone app.
A company’s product ecosystem may also spread out across multiple apps.
If a company needs to understand a user’s activity across all apps, Segment recommends connecting all sources to the same Space. This provides a comprehensive view of a user’s activity across the entire app ecosystem.
If, however, each app should maintain its own metrics and LTV analysis, regardless of the overlap of users between apps, Segment recommends creating a separate Space per app and only connecting sources related to each app to its space. This will give a siloed view of how users interact with each individual app.
Each workspace has two spaces by default. Contact your CSM to enable additional spaces.
To learn more, visit Segment’seCommerce Example doc.
A user can interact with a brand through multiple channels and departments. A user might have touch points with a sales team, a marketing team, and a customer support team throughout their customer journey. It’s important for companies to have insights into these cross-functional activities to ensure they understand the complete customer experience.
For example, if a user has logged a complaint with a customer support team, the marketing team should exclude this user from an automatic follow-up email asking for them to leave a public product review on their site.
This page was last modified: 28 Mar 2023
Questions? Problems? Need more info? Contact Segment Support for assistance!
Thanks for your feedback!

Headings:
Identity Resolution Use-Cases
Anonymous to known identification
Cross-device identification
Cross-app identification
Cross-Channel identification
Need support?
Help improve these docs!
Was this page helpful?
Get started with Segment
Crawling: https://segment.com/docs/unify/identity-resolution/externalids/
Paragraphs:
Unify requires a Business tier account and is included with Engage.See theavailable plans, orcontact Support.
On this page
The steps in this guide pertain to spaces created before September 27th, 2020. For spaces created after September 27th, 2020, please refer to theIdentity onboarding guide.
The Identity Graph creates or merges profiles based on externalIDs. ExternalIDs will become the identities attached to a user profile in the Profile explorer.
Navigate toUnify > Profile explorerto view identities attached to a profile, along with custom traits, event history, and more.

Segment automatically promotes the following traits and IDs in track and identify calls to externalIDs:
The Google clientID(ga_clientid) is a unique value created for each browser-device pair and will exist for 2 years if the cookie is not cleared. The analytics.reset() call should be triggered from Segment end when the user logs off. This call will clear the cookies and local Storage created by Segment. It doesn’t clear data from other integrated tools. So on the next login, the user will be assigned with a new unique anonymous_id, but the same ga_clientid will remain if this cookie is not cleared. Hence, the profiles with different anonymous_id but with same ga_clientid will get merged.
Unify resolves identity for any other externalIDs that you bind to users - such as a phone number or any custom identifier that you support.
As long as you’ve configured custom externalIDs, such asphone, in your Space’s Identity Resolution rules, you can include it with thecontext.externalIdsarray, thepropertiesobject, or thecontext.traitsobject.
As seen in the example below, you can send customexternalIdsin thecontextobject of any call to Segment’s API.
The four fields below (id, type, collection, encoding) are all required:
As an example:
Additionally, addingphonewith thepropertiesobject gets picked up by Unify and applied as an externalID:
You can also includephoneusing thecontext.traitsobject and Unify adds it as an externalID to the profile.
Unify creates a user (user_id:use_123)  with the custom externalID (phone:123-456-7890). Query the user’s phone record by using the externalID (phone:123-456-7890), or update the profile with that externalID going forward. (Note: externalIDs must be lower-case.)
Users can view which externalIDs are promoted on each event by viewing the raw payload on Events in the User Profile in the “external_ids” object.
For example, the following user had anonymous_id and user_id promoted as identifiers from the Course Clicked track call:

For example, a new anonymous user visits your Pricing page:
At this point, the Identity Graph will create a new user with external id (anonymous_id:anon_123) and a persistent and globally unique segment_id, in this case:use_4paotyretuj4Ta2bEYQ0vKOq1e7.

Any new events received with the same external id (anonymous_id:anon_123) are appended to same useruse_4paotyretuj4Ta2bEYQ0vKOq1e7.
Next, the user goes to a sign up form and signs up:
At this point, the Identity Graph associates external ID (user_id:use_123) with the same useruse_4paotyretuj4Ta2bEYQ0vKOq1e7.